[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematics of Data Science",
    "section": "",
    "text": "Mathematics of Data Science"
  },
  {
    "objectID": "content/06-clustering---graph-cuts.html",
    "href": "content/06-clustering---graph-cuts.html",
    "title": "Graph Cuts",
    "section": "",
    "text": "Code\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nfrom io import BytesIO\nfrom PIL import Image\nimport gradio as gr\nimport pandas as pd"
  },
  {
    "objectID": "content/06-clustering---graph-cuts.html#introduction",
    "href": "content/06-clustering---graph-cuts.html#introduction",
    "title": "Graph Cuts",
    "section": "1 Introduction",
    "text": "1 Introduction\nGraph-based clustering is a powerful approach for segmenting a graph into different subsets (clusters) where nodes within the same cluster are more densely connected. One common method for graph-based clustering is the use of graph cuts, which splits the graph into disjoint subsets while minimizing the cut (sum of weights of the edges cut).\nThis notebook demonstrates the concept of normalized graph cuts, a specific type of graph cut that aims to partition a graph while taking into account the degree of each cluster, which helps in maintaining balanced clusters.\nHere, we visualize and analyze the graph partitioning process on a simple graph using normalized cuts."
  },
  {
    "objectID": "content/06-clustering---graph-cuts.html#python-implementation",
    "href": "content/06-clustering---graph-cuts.html#python-implementation",
    "title": "Graph Cuts",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\nLetâ€™s start by defining a function to create a graph and visualize its partition:\n\n\nCode\ndef create_graph() -&gt; nx.Graph:\n    \"\"\"Create and return the graph with weights.\"\"\"\n    G = nx.Graph()\n    edges = [(1, 2, 3), (1, 3, 1), (2, 4, 1), (3, 4, 3)]\n    G.add_weighted_edges_from(edges)\n    return G\n\n\ndef visualize_partition(\n    G: nx.Graph,\n    partition: list[set[int]],\n    ncut_value: float,\n    combination_num: int | str,\n    total_combinations: int,\n) -&gt; Image.Image:\n    \"\"\"Enhanced visualization with professional styling\"\"\"\n    plt.figure(figsize=(10, 8), dpi=100)\n\n    pos = {\n        1: (0, 0),  # bottom (v1)\n        2: (-1, 1),  # left (v2)\n        3: (1, 1),  # right (v3)\n        4: (0, 2),  # top (v4)\n    }\n\n    colors = [\"#1f77b4\" if node in partition[0] else \"#ff7f0e\" for node in G.nodes()]\n\n    nx.draw_networkx_nodes(\n        G, pos, node_color=colors, node_size=1200, edgecolors=\"black\", linewidths=2\n    )\n\n    nx.draw_networkx_edges(G, pos, width=2, alpha=0.8, edge_color=\"gray\")\n\n    edge_labels = nx.get_edge_attributes(G, \"weight\")\n    nx.draw_networkx_edge_labels(\n        G,\n        pos,\n        edge_labels=edge_labels,\n        font_size=12,\n        label_pos=0.75,\n        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.9),\n    )\n\n    labels = {node: f\"v{node}\" for node in G.nodes()}\n    nx.draw_networkx_labels(\n        G, pos, labels, font_size=14, font_weight=\"bold\", font_family=\"sans-serif\"\n    )\n\n    plt.title(\n        f\"Partition Analysis: Combination {combination_num}/{total_combinations}\\n\"\n        f\"NCut Value: {ncut_value:.4f}\",\n        fontsize=14,\n        pad=20,\n    )\n\n    plt.margins(0.15)\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n    buf.seek(0)\n    image = Image.open(buf).convert(\"RGB\")\n    plt.close()\n    return image\n\n\nThe following function calculates the normalized cut for a given partition, which helps us find the optimal partition with the minimal normalized cut value:\n\n\nCode\ndef calculate_normalized_cut(\n    G: nx.Graph, subset1: set[int], subset2: set[int]\n) -&gt; float:\n    \"\"\"Calculate normalized cut value for given partition\"\"\"\n    cut_value = sum(G[u][v][\"weight\"] for u in subset1 for v in subset2 if v in G[u])\n\n    vol1 = sum(dict(G.degree(weight=\"weight\"))[v] for v in subset1)\n    vol2 = sum(dict(G.degree(weight=\"weight\"))[v] for v in subset2)\n\n    if vol1 == 0 or vol2 == 0:\n        return float(\"inf\")\n\n    return (cut_value / vol1) + (cut_value / vol2)"
  },
  {
    "objectID": "content/06-clustering---graph-cuts.html#interactive-dashboard",
    "href": "content/06-clustering---graph-cuts.html#interactive-dashboard",
    "title": "Graph Cuts",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\nThe Gradio-powered dashboard enables interaction with the graph partitioning analysis, allowing users to view all possible partitions and identify the optimal one based on the normalized cut value:\n\n\nCode\ndef display_all_combinations_gradio(\n    G: nx.Graph,\n) -&gt; tuple[list[tuple[Image.Image, str]], pd.DataFrame, dict]:\n    \"\"\"\n    Returns:\n        gallery_items: List of (image, caption)\n        results_df: DataFrame with all results\n        best_partition_info: Dictionary with best partition details\n    \"\"\"\n    vertices = set(G.nodes())\n    all_results = []\n\n    for size in range(1, len(vertices)):\n        for subset1 in combinations(vertices, size):\n            subset1_set = set(subset1)\n            subset2 = vertices - subset1_set\n            ncut = calculate_normalized_cut(G, subset1_set, subset2)\n\n            all_results.append(\n                {\n                    \"partition_1\": sorted(subset1_set),\n                    \"partition_2\": sorted(subset2),\n                    \"ncut\": ncut,\n                    \"cut_value\": sum(\n                        G[u][v][\"weight\"]\n                        for u in subset1_set\n                        for v in subset2\n                        if v in G[u]\n                    ),\n                }\n            )\n\n    results_df = pd.DataFrame(all_results)\n    results_df[\"combination\"] = results_df.index + 1\n    results_df = results_df.sort_values(\"ncut\").reset_index(drop=True)\n\n    best = results_df.iloc[0]\n    best_partition_info = {\n        \"partitions\": [best[\"partition_1\"], best[\"partition_2\"]],\n        \"ncut\": best[\"ncut\"],\n        \"cut_value\": best[\"cut_value\"],\n    }\n\n    gallery_items = []\n    for idx, row in results_df.iterrows():\n        caption = (\n            f\"Combination {idx+1}\\n\"\n            f\"NCut: {row['ncut']:.4f}\\n\"\n            f\"Partitions: {row['partition_1']} | {row['partition_2']}\"\n        )\n        img = visualize_partition(\n            G,\n            [set(row[\"partition_1\"]), set(row[\"partition_2\"])],\n            row[\"ncut\"],\n            idx + 1,\n            len(results_df),\n        )\n        gallery_items.append((img, caption))\n\n    return gallery_items, results_df, best_partition_info\n\n\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"Graph Partition Analysis\",\n) as demo:\n    with gr.Row():\n        graph_img = gr.Image(label=\"Base Graph\", interactive=False)\n        best_partition_img = gr.Image(label=\"Best Partition\", interactive=False)\n\n    with gr.Row():\n        with gr.Column(scale=3):\n            with gr.Tabs():\n                with gr.TabItem(\"All Partitions\"):\n                    gallery = gr.Gallery(\n                        label=\"Partition Visualizations\",\n                        show_label=True,\n                        columns=3,\n                        rows=4,\n                        object_fit=\"contain\",\n                        height=\"auto\",\n                    )\n\n                with gr.TabItem(\"Analysis Results\"):\n                    results_table = gr.Dataframe(\n                        headers=[\n                            \"Combination\",\n                            \"Partition 1\",\n                            \"Partition 2\",\n                            \"NCut Value\",\n                            \"Cut Value\",\n                        ],\n                        datatype=[\"number\", \"str\", \"str\", \"number\", \"number\"],\n                        interactive=False,\n                        wrap=True,\n                    )\n\n    best_partition_details = gr.JSON(label=\"Optimization Results\")\n\n    # Define the main analysis function\n    def run_analysis():\n        G = create_graph()\n\n        # Visualize base graph\n        base_img = visualize_partition(G, [set(), set(G.nodes())], 0, 0, 0)\n\n        # Get results\n        gallery_items, results_df, best_info = display_all_combinations_gradio(G)\n\n        # Format results for display\n        display_df = results_df[\n            [\"combination\", \"partition_1\", \"partition_2\", \"ncut\", \"cut_value\"]\n        ]\n        display_df.columns = [\n            \"Combination\",\n            \"Partition 1\",\n            \"Partition 2\",\n            \"NCut Value\",\n            \"Cut Value\",\n        ]\n\n        # Create best partition visualization\n        best_img = visualize_partition(\n            G, best_info[\"partitions\"], best_info[\"ncut\"], \"Best\", len(results_df)\n        )\n\n        return [\n            base_img,\n            gallery_items,\n            display_df,\n            best_img,\n            {\n                \"Normalized Cut Value\": best_info[\"ncut\"],\n                \"Cut Value\": best_info[\"cut_value\"],\n                \"Partitions\": best_info[\"partitions\"],\n            },\n        ]\n\n    demo.load(\n        fn=run_analysis,\n        outputs=[\n            graph_img,\n            gallery,\n            results_table,\n            best_partition_img,\n            best_partition_details,\n        ],\n    )\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nfrom io import BytesIO\nfrom PIL import Image\nimport gradio as gr\nimport pandas as pd\ndef create_graph() -&gt; nx.Graph:\n    \"\"\"Create and return the graph with weights.\"\"\"\n    G = nx.Graph()\n    edges = [(1, 2, 3), (1, 3, 1), (2, 4, 1), (3, 4, 3)]\n    G.add_weighted_edges_from(edges)\n    return G\n\n\ndef visualize_partition(\n    G: nx.Graph,\n    partition: list[set[int]],\n    ncut_value: float,\n    combination_num: int | str,\n    total_combinations: int,\n) -&gt; Image.Image:\n    \"\"\"Enhanced visualization with professional styling\"\"\"\n    plt.figure(figsize=(10, 8), dpi=100)\n\n    pos = {\n        1: (0, 0),  # bottom (v1)\n        2: (-1, 1),  # left (v2)\n        3: (1, 1),  # right (v3)\n        4: (0, 2),  # top (v4)\n    }\n\n    colors = [\"#1f77b4\" if node in partition[0] else \"#ff7f0e\" for node in G.nodes()]\n\n    nx.draw_networkx_nodes(\n        G, pos, node_color=colors, node_size=1200, edgecolors=\"black\", linewidths=2\n    )\n\n    nx.draw_networkx_edges(G, pos, width=2, alpha=0.8, edge_color=\"gray\")\n\n    edge_labels = nx.get_edge_attributes(G, \"weight\")\n    nx.draw_networkx_edge_labels(\n        G,\n        pos,\n        edge_labels=edge_labels,\n        font_size=12,\n        label_pos=0.75,\n        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.9),\n    )\n\n    labels = {node: f\"v{node}\" for node in G.nodes()}\n    nx.draw_networkx_labels(\n        G, pos, labels, font_size=14, font_weight=\"bold\", font_family=\"sans-serif\"\n    )\n\n    plt.title(\n        f\"Partition Analysis: Combination {combination_num}/{total_combinations}\\n\"\n        f\"NCut Value: {ncut_value:.4f}\",\n        fontsize=14,\n        pad=20,\n    )\n\n    plt.margins(0.15)\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n    buf.seek(0)\n    image = Image.open(buf).convert(\"RGB\")\n    plt.close()\n    return image\ndef calculate_normalized_cut(\n    G: nx.Graph, subset1: set[int], subset2: set[int]\n) -&gt; float:\n    \"\"\"Calculate normalized cut value for given partition\"\"\"\n    cut_value = sum(G[u][v][\"weight\"] for u in subset1 for v in subset2 if v in G[u])\n\n    vol1 = sum(dict(G.degree(weight=\"weight\"))[v] for v in subset1)\n    vol2 = sum(dict(G.degree(weight=\"weight\"))[v] for v in subset2)\n\n    if vol1 == 0 or vol2 == 0:\n        return float(\"inf\")\n\n    return (cut_value / vol1) + (cut_value / vol2)\ndef display_all_combinations_gradio(\n    G: nx.Graph,\n) -&gt; tuple[list[tuple[Image.Image, str]], pd.DataFrame, dict]:\n    \"\"\"\n    Returns:\n        gallery_items: List of (image, caption)\n        results_df: DataFrame with all results\n        best_partition_info: Dictionary with best partition details\n    \"\"\"\n    vertices = set(G.nodes())\n    all_results = []\n\n    for size in range(1, len(vertices)):\n        for subset1 in combinations(vertices, size):\n            subset1_set = set(subset1)\n            subset2 = vertices - subset1_set\n            ncut = calculate_normalized_cut(G, subset1_set, subset2)\n\n            all_results.append(\n                {\n                    \"partition_1\": sorted(subset1_set),\n                    \"partition_2\": sorted(subset2),\n                    \"ncut\": ncut,\n                    \"cut_value\": sum(\n                        G[u][v][\"weight\"]\n                        for u in subset1_set\n                        for v in subset2\n                        if v in G[u]\n                    ),\n                }\n            )\n\n    results_df = pd.DataFrame(all_results)\n    results_df[\"combination\"] = results_df.index + 1\n    results_df = results_df.sort_values(\"ncut\").reset_index(drop=True)\n\n    best = results_df.iloc[0]\n    best_partition_info = {\n        \"partitions\": [best[\"partition_1\"], best[\"partition_2\"]],\n        \"ncut\": best[\"ncut\"],\n        \"cut_value\": best[\"cut_value\"],\n    }\n\n    gallery_items = []\n    for idx, row in results_df.iterrows():\n        caption = (\n            f\"Combination {idx+1}\\n\"\n            f\"NCut: {row['ncut']:.4f}\\n\"\n            f\"Partitions: {row['partition_1']} | {row['partition_2']}\"\n        )\n        img = visualize_partition(\n            G,\n            [set(row[\"partition_1\"]), set(row[\"partition_2\"])],\n            row[\"ncut\"],\n            idx + 1,\n            len(results_df),\n        )\n        gallery_items.append((img, caption))\n\n    return gallery_items, results_df, best_partition_info\n\n\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"Graph Partition Analysis\",\n) as demo:\n    with gr.Row():\n        graph_img = gr.Image(label=\"Base Graph\", interactive=False)\n        best_partition_img = gr.Image(label=\"Best Partition\", interactive=False)\n\n    with gr.Row():\n        with gr.Column(scale=3):\n            with gr.Tabs():\n                with gr.TabItem(\"All Partitions\"):\n                    gallery = gr.Gallery(\n                        label=\"Partition Visualizations\",\n                        show_label=True,\n                        columns=3,\n                        rows=4,\n                        object_fit=\"contain\",\n                        height=\"auto\",\n                    )\n\n                with gr.TabItem(\"Analysis Results\"):\n                    results_table = gr.Dataframe(\n                        headers=[\n                            \"Combination\",\n                            \"Partition 1\",\n                            \"Partition 2\",\n                            \"NCut Value\",\n                            \"Cut Value\",\n                        ],\n                        datatype=[\"number\", \"str\", \"str\", \"number\", \"number\"],\n                        interactive=False,\n                        wrap=True,\n                    )\n\n    best_partition_details = gr.JSON(label=\"Optimization Results\")\n\n    # Define the main analysis function\n    def run_analysis():\n        G = create_graph()\n\n        # Visualize base graph\n        base_img = visualize_partition(G, [set(), set(G.nodes())], 0, 0, 0)\n\n        # Get results\n        gallery_items, results_df, best_info = display_all_combinations_gradio(G)\n\n        # Format results for display\n        display_df = results_df[\n            [\"combination\", \"partition_1\", \"partition_2\", \"ncut\", \"cut_value\"]\n        ]\n        display_df.columns = [\n            \"Combination\",\n            \"Partition 1\",\n            \"Partition 2\",\n            \"NCut Value\",\n            \"Cut Value\",\n        ]\n\n        # Create best partition visualization\n        best_img = visualize_partition(\n            G, best_info[\"partitions\"], best_info[\"ncut\"], \"Best\", len(results_df)\n        )\n\n        return [\n            base_img,\n            gallery_items,\n            display_df,\n            best_img,\n            {\n                \"Normalized Cut Value\": best_info[\"ncut\"],\n                \"Cut Value\": best_info[\"cut_value\"],\n                \"Partitions\": best_info[\"partitions\"],\n            },\n        ]\n\n    demo.load(\n        fn=run_analysis,\n        outputs=[\n            graph_img,\n            gallery,\n            results_table,\n            best_partition_img,\n            best_partition_details,\n        ],\n    )\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/03-image-processing---contrast.html",
    "href": "content/03-image-processing---contrast.html",
    "title": "Contrast Enhancement via Histogram Equalization",
    "section": "",
    "text": "Code\nimport numpy as np\nfrom PIL import Image\nimport plotly.graph_objs as go\nimport gradio as gr\nfrom skimage import img_as_float, data"
  },
  {
    "objectID": "content/03-image-processing---contrast.html#introduction",
    "href": "content/03-image-processing---contrast.html#introduction",
    "title": "Contrast Enhancement via Histogram Equalization",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this notebook, we demonstrate the technique of contrast enhancement using histogram equalization. This method is particularly effective for improving the visual quality of images with low contrast, where the histogram is uneven and limited to a narrow range.\nThe primary aim is to redistribute the intensity values to achieve a more uniform histogram, thereby enhancing contrast. This transformation seeks to use the full range of pixel values more effectively:\n\nConceptual Foundation:\n\nIf \\(X : [0, a] \\to \\mathbb{R}_+\\) is a random variable with a continuous, positive density \\(g\\),\nThe transformed variable \\(Y = \\int_{0}^{X} g(t) \\, dt\\) will be uniformly distributed in \\([0,1]\\).\nFor images, we employ a discrete version of this method.\n\nHistogram Equalization Formula: Given an image with a histogram \\(h(k)\\) for \\(k = 0, \\dots, 255\\) and total pixel count \\(N\\):\n\nThe enhanced intensity at pixel \\((x,y)\\) is expressed as:\n\n\\[\nI_{\\text{enhanced}}(x,y) = \\left\\lfloor 255 \\cdot \\sum_{k=0}^{I(x,y)} \\frac{h(k)}{N} \\right\\rfloor.\n\\]\n\nThis notebook accomplishes these transformations using pure Python functions and provides an interactive Gradio dashboard to visualize results.\nThe interactive dashboard features: - Side-by-side display of the original and enhanced images. - Responsive histograms for both images using Plotly. - Detailed histogram data in a JSON format for further analysis."
  },
  {
    "objectID": "content/03-image-processing---contrast.html#python-implementation",
    "href": "content/03-image-processing---contrast.html#python-implementation",
    "title": "Contrast Enhancement via Histogram Equalization",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\nBelow is the implementation of histogram equalization, which enhances the contrast by redistributing pixel intensity values:\n\n\nCode\ndef contrast_enhancer(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Enhance the contrast of a grayscale image using histogram equalization.\n\n    The function computes the normalized cumulative histogram (CDF) and maps each pixel\n    to its new intensity value based on the CDF.\n\n    Args:\n        img (np.ndarray): Input grayscale image (uint8, values in 0-255).\n\n    Returns:\n        np.ndarray: Contrast-enhanced image (uint8).\n    \"\"\"\n    # Compute the histogram: count of each pixel intensity from 0 to 255\n    hist = np.bincount(img.ravel(), minlength=256)\n\n    # Normalize the histogram to obtain the probability distribution\n    prob = hist / img.size\n\n    # Compute the cumulative distribution function (CDF)\n    cdf = prob.cumsum()\n\n    # Map original intensities through the CDF and scale to [0,255]\n    equalized_img = np.round(cdf[img] * 255).astype(np.uint8)\n\n    return equalized_img\n\n\nThe following function creates a responsive Plotly histogram to visualize intensity distributions of both the original and enhanced images:\n\n\nCode\ndef plot_histogram(img: np.ndarray, title: str = \"Histogram\") -&gt; go.Figure:\n    \"\"\"\n    Generate a responsive Plotly bar chart of the image's histogram.\n\n    Args:\n        img (np.ndarray): Grayscale image.\n        title (str): Title of the plot.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n    # Compute histogram: count of each intensity value\n    hist = np.bincount(img.ravel(), minlength=256)\n    x = list(range(256))\n\n    # Create a responsive Plotly bar chart\n    fig = go.Figure(data=[go.Bar(x=x, y=hist, marker_color=\"steelblue\")])\n    fig.update_layout(\n        title=title,\n        xaxis=dict(title=\"Pixel Intensity\", tickmode=\"linear\", dtick=10),\n        yaxis=dict(title=\"Count\"),\n        margin=dict(l=40, r=40, t=40, b=40),\n        template=\"plotly_white\",\n        autosize=True,\n    )\n    return fig"
  },
  {
    "objectID": "content/03-image-processing---contrast.html#interactive-dashboard",
    "href": "content/03-image-processing---contrast.html#interactive-dashboard",
    "title": "Contrast Enhancement via Histogram Equalization",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\nThe Gradio dashboard below allows interaction with images for contrast enhancement through histogram equalization. It includes:\n\nOriginal and Enhanced Image Viewer: Enables direct comparison.\nPlotly-based Dynamic Histograms: Displays intensity distribution before and after enhancement.\nJSON Statistics: Provides quantitative insights into histogram changes.\n\n\n\nCode\nDEFAULT_IMAGE = img_as_float(data.camera())\n\n\n\n\nCode\ndef process_image(\n    image: Image.Image,\n) -&gt; tuple[Image.Image, Image.Image, go.Figure, go.Figure, dict]:\n    \"\"\"\n    Process the input image by converting it to grayscale, enhancing its contrast,\n    and generating responsive histograms (using Plotly) for both the original and enhanced images.\n\n    Args:\n        image (Image.Image): Input image (in any mode; will be converted to grayscale).\n\n    Returns:\n        tuple:\n            - Original Image (PIL.Image): The grayscale version of the input image.\n            - Enhanced Image (PIL.Image): The contrast-enhanced image.\n            - Original Histogram (go.Figure): Responsive Plotly figure of the original histogram.\n            - Equalized Histogram (go.Figure): Responsive Plotly figure of the equalized histogram.\n            - Histogram Data (dict): Detailed histogram statistics.\n    \"\"\"\n    # Convert to grayscale\n    img_gray = image.convert(\"L\")\n    img_array = np.array(img_gray)\n\n    # Generate histogram for the original image using Plotly\n    fig_original = plot_histogram(img_array, title=\"Original Histogram\")\n\n    # Enhance contrast using histogram equalization\n    enhanced_array = contrast_enhancer(img_array)\n    enhanced_image = Image.fromarray(enhanced_array)\n\n    # Generate histogram for the enhanced image using Plotly\n    fig_enhanced = plot_histogram(enhanced_array, title=\"Equalized Histogram\")\n\n    # Prepare detailed histogram data (JSON-friendly)\n    original_hist = np.bincount(img_array.ravel(), minlength=256).tolist()\n    enhanced_hist = np.bincount(enhanced_array.ravel(), minlength=256).tolist()\n    histogram_data = {\n        \"original_histogram\": original_hist,\n        \"enhanced_histogram\": enhanced_hist,\n        \"original_stats\": {\n            \"min\": int(np.min(img_array)),\n            \"max\": int(np.max(img_array)),\n            \"mean\": float(np.mean(img_array)),\n        },\n        \"enhanced_stats\": {\n            \"min\": int(np.min(enhanced_array)),\n            \"max\": int(np.max(enhanced_array)),\n            \"mean\": float(np.mean(enhanced_array)),\n        },\n    }\n\n    return (img_gray, enhanced_image, fig_original, fig_enhanced, histogram_data)\n\n\nwith gr.Blocks(css=\"\"\"gradio-app {background: #222222 !important}\"\"\") as demo:\n    gr.Markdown(\n        \"\"\"\n        # Contrast Enhancement and Histogram Equalization\n        Upload an image to perform contrast enhancement via histogram equalization.\n        \"\"\"\n    )\n\n    input_image = gr.Image(label=\"Input Image\", type=\"pil\", value=DEFAULT_IMAGE)\n    submit_button = gr.Button(\"Enhance Contrast\")\n\n    with gr.Row():\n        original_image = gr.Image(label=\"Original Image\")\n        enhanced_image = gr.Image(label=\"Enhanced Image\")\n\n    original_hist = gr.Plot(label=\"Original Histogram\")\n    equalized_hist = gr.Plot(label=\"Equalized Histogram\")\n\n    # JSON output placed in a narrow row to minimize dashboard height.\n    hist_json = gr.JSON(label=\"Histogram Data\", elem_classes=\"gr-json\")\n\n    submit_button.click(\n        fn=process_image,\n        inputs=input_image,\n        outputs=[\n            original_image,\n            enhanced_image,\n            original_hist,\n            equalized_hist,\n            hist_json,\n        ],\n    )\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport numpy as np\nfrom PIL import Image\nimport plotly.graph_objs as go\nimport gradio as gr\nfrom skimage import img_as_float, data\ndef contrast_enhancer(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Enhance the contrast of a grayscale image using histogram equalization.\n\n    The function computes the normalized cumulative histogram (CDF) and maps each pixel\n    to its new intensity value based on the CDF.\n\n    Args:\n        img (np.ndarray): Input grayscale image (uint8, values in 0-255).\n\n    Returns:\n        np.ndarray: Contrast-enhanced image (uint8).\n    \"\"\"\n    # Compute the histogram: count of each pixel intensity from 0 to 255\n    hist = np.bincount(img.ravel(), minlength=256)\n\n    # Normalize the histogram to obtain the probability distribution\n    prob = hist / img.size\n\n    # Compute the cumulative distribution function (CDF)\n    cdf = prob.cumsum()\n\n    # Map original intensities through the CDF and scale to [0,255]\n    equalized_img = np.round(cdf[img] * 255).astype(np.uint8)\n\n    return equalized_img\ndef plot_histogram(img: np.ndarray, title: str = \"Histogram\") -&gt; go.Figure:\n    \"\"\"\n    Generate a responsive Plotly bar chart of the image's histogram.\n\n    Args:\n        img (np.ndarray): Grayscale image.\n        title (str): Title of the plot.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n    # Compute histogram: count of each intensity value\n    hist = np.bincount(img.ravel(), minlength=256)\n    x = list(range(256))\n\n    # Create a responsive Plotly bar chart\n    fig = go.Figure(data=[go.Bar(x=x, y=hist, marker_color=\"steelblue\")])\n    fig.update_layout(\n        title=title,\n        xaxis=dict(title=\"Pixel Intensity\", tickmode=\"linear\", dtick=10),\n        yaxis=dict(title=\"Count\"),\n        margin=dict(l=40, r=40, t=40, b=40),\n        template=\"plotly_white\",\n        autosize=True,\n    )\n    return fig\nDEFAULT_IMAGE = img_as_float(data.camera())\ndef process_image(\n    image: Image.Image,\n) -&gt; tuple[Image.Image, Image.Image, go.Figure, go.Figure, dict]:\n    \"\"\"\n    Process the input image by converting it to grayscale, enhancing its contrast,\n    and generating responsive histograms (using Plotly) for both the original and enhanced images.\n\n    Args:\n        image (Image.Image): Input image (in any mode; will be converted to grayscale).\n\n    Returns:\n        tuple:\n            - Original Image (PIL.Image): The grayscale version of the input image.\n            - Enhanced Image (PIL.Image): The contrast-enhanced image.\n            - Original Histogram (go.Figure): Responsive Plotly figure of the original histogram.\n            - Equalized Histogram (go.Figure): Responsive Plotly figure of the equalized histogram.\n            - Histogram Data (dict): Detailed histogram statistics.\n    \"\"\"\n    # Convert to grayscale\n    img_gray = image.convert(\"L\")\n    img_array = np.array(img_gray)\n\n    # Generate histogram for the original image using Plotly\n    fig_original = plot_histogram(img_array, title=\"Original Histogram\")\n\n    # Enhance contrast using histogram equalization\n    enhanced_array = contrast_enhancer(img_array)\n    enhanced_image = Image.fromarray(enhanced_array)\n\n    # Generate histogram for the enhanced image using Plotly\n    fig_enhanced = plot_histogram(enhanced_array, title=\"Equalized Histogram\")\n\n    # Prepare detailed histogram data (JSON-friendly)\n    original_hist = np.bincount(img_array.ravel(), minlength=256).tolist()\n    enhanced_hist = np.bincount(enhanced_array.ravel(), minlength=256).tolist()\n    histogram_data = {\n        \"original_histogram\": original_hist,\n        \"enhanced_histogram\": enhanced_hist,\n        \"original_stats\": {\n            \"min\": int(np.min(img_array)),\n            \"max\": int(np.max(img_array)),\n            \"mean\": float(np.mean(img_array)),\n        },\n        \"enhanced_stats\": {\n            \"min\": int(np.min(enhanced_array)),\n            \"max\": int(np.max(enhanced_array)),\n            \"mean\": float(np.mean(enhanced_array)),\n        },\n    }\n\n    return (img_gray, enhanced_image, fig_original, fig_enhanced, histogram_data)\n\n\nwith gr.Blocks(css=\"\"\"gradio-app {background: #222222 !important}\"\"\") as demo:\n    gr.Markdown(\n        \"\"\"\n        # Contrast Enhancement and Histogram Equalization\n        Upload an image to perform contrast enhancement via histogram equalization.\n        \"\"\"\n    )\n\n    input_image = gr.Image(label=\"Input Image\", type=\"pil\", value=DEFAULT_IMAGE)\n    submit_button = gr.Button(\"Enhance Contrast\")\n\n    with gr.Row():\n        original_image = gr.Image(label=\"Original Image\")\n        enhanced_image = gr.Image(label=\"Enhanced Image\")\n\n    original_hist = gr.Plot(label=\"Original Histogram\")\n    equalized_hist = gr.Plot(label=\"Equalized Histogram\")\n\n    # JSON output placed in a narrow row to minimize dashboard height.\n    hist_json = gr.JSON(label=\"Histogram Data\", elem_classes=\"gr-json\")\n\n    submit_button.click(\n        fn=process_image,\n        inputs=input_image,\n        outputs=[\n            original_image,\n            enhanced_image,\n            original_hist,\n            equalized_hist,\n            hist_json,\n        ],\n    )\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/05-dimension-reduction.html",
    "href": "content/05-dimension-reduction.html",
    "title": "Dimension Reduction",
    "section": "",
    "text": "Code\nimport gradio as gr\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.datasets import load_breast_cancer, load_digits, load_iris, load_wine\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "content/05-dimension-reduction.html#introduction",
    "href": "content/05-dimension-reduction.html#introduction",
    "title": "Dimension Reduction",
    "section": "1 Introduction",
    "text": "1 Introduction\nHigh-dimensional data often contains redundancy, noise, and dependencies between features, making it challenging to analyze efficiently. Dimension reduction techniques aim to transform data from a high-dimensional space \\(\\mathbb{R}^d\\) to a lower-dimensional representation \\(\\mathbb{R}^s\\) while preserving its essential characteristics.\nTwo common approaches to dimension reduction are:\n\nLinear methods: Principal Component Analysis (PCA)\nNon-linear methods: Kernel Principal Component Analysis (Kernel PCA)\n\nThis notebook explores both methods and provides an interactive tool to analyze real-world datasets."
  },
  {
    "objectID": "content/05-dimension-reduction.html#principal-component-analysis-pca",
    "href": "content/05-dimension-reduction.html#principal-component-analysis-pca",
    "title": "Dimension Reduction",
    "section": "2 Principal Component Analysis (PCA)",
    "text": "2 Principal Component Analysis (PCA)\nGiven a dataset \\(X = (x_1, \\dots, x_n) \\in \\mathbb{R}^{d \\times n}\\), we assume zero mean \\(\\bar{x} = 0\\). The goal of PCA is to find a new orthonormal basis that maximizes the variance of the projected data.\n\nCompute the covariance matrix: \\[\nC = \\frac{1}{n} X X^T \\in \\mathbb{R}^{d \\times d}\n\\]\nSolve the eigenvalue problem:\n\n\\[\nC v_k = \\lambda_k v_k\n\\]\nwhere \\(\\lambda_k\\) and \\(v_k\\) are eigenvalues and eigenvectors of \\(C\\).\n\nSelect the top \\(s\\) eigenvectors \\(V = (v_1, \\dots, v_s)\\) to form the projection matrix.\nCompute the dimension-reduced data:\n\n\\[\nZ = V^T X \\in \\mathbb{R}^{s \\times n}\n\\]\n\n2.1 Properties\n\nThe variance of the projected data is maximized.\nThe total variance explained by the top \\(s\\) components is: \\[\n\\text{Var}(V^T X) = \\lambda_1 + \\dots + \\lambda_s\n\\]\nThe approximation error of reconstructing \\(X\\) from \\(Z\\) is minimized.\n\n\nWe apply PCA to datasets such as Iris, Digits, Breast Cancer, and Wine and visualize the low-dimensional representation."
  },
  {
    "objectID": "content/05-dimension-reduction.html#kernel-principal-component-analysis-kernel-pca",
    "href": "content/05-dimension-reduction.html#kernel-principal-component-analysis-kernel-pca",
    "title": "Dimension Reduction",
    "section": "3 Kernel Principal Component Analysis (Kernel PCA)",
    "text": "3 Kernel Principal Component Analysis (Kernel PCA)\nPCA is limited to linear transformations, which might fail when data is non-linearly separable. Kernel PCA overcomes this by mapping data into a high-dimensional feature space \\(\\mathbb{R}^D\\) using a kernel function \\(k(x, y)\\), and then applying PCA in that space.\n\nDefine a feature map \\(\\Phi: \\mathbb{R}^d \\to \\mathbb{R}^D\\) that implicitly transforms the data:\n\n\\[\n\\phi_i = \\Phi(x_i) \\in \\mathbb{R}^D\n\\]\n\nCompute the kernel matrix:\n\n\\[\nK_{ij} = k(x_i, x_j) = \\langle \\Phi(x_i), \\Phi(x_j) \\rangle\n\\]\n\nCenter the kernel matrix:\n\n\\[\n\\tilde{K} = K - \\frac{1}{n} K 1_n - \\frac{1}{n} 1_n K + \\frac{1}{n^2} 1_n K 1_n\n\\]\n\nSolve the eigenvalue problem:\n\n\\[\nn \\lambda_k \\alpha_k = \\tilde{K} \\alpha_k\n\\]\n\nCompute the principal components:\n\n\\[\nZ = (\\alpha_1, \\dots, \\alpha_s)^T \\tilde{K}\n\\]\nCommon Kernels:\n\nLinear kernel: \\(k(x, y) = \\langle x, y \\rangle\\)\nPolynomial kernel: \\(k(x, y) = (\\langle x, y \\rangle + 1)^d\\)\nRBF (Gaussian) kernel: \\(k(x, y) = e^{-\\gamma \\|x - y\\|^2}\\)\n\n\nKernel PCA is applied to datasets to reveal non-linear structures in data. The interactive tool allows users to compare PCA and Kernel PCA with different kernels."
  },
  {
    "objectID": "content/05-dimension-reduction.html#python-implementation",
    "href": "content/05-dimension-reduction.html#python-implementation",
    "title": "Dimension Reduction",
    "section": "4 Python Implementation",
    "text": "4 Python Implementation\n\n\nCode\ndef svd_low_rank_approximation(A: np.ndarray, p: int) -&gt; np.ndarray:\n    \"\"\"Compute pseudo-inverse using low-rank SVD approximation\"\"\"\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    S_inv = np.diag(1 / s[:p])\n    return Vt[:p].T @ S_inv @ U[:, :p].T\n\n\ndef compute_pca(X: np.ndarray, s: int) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"PCA implementation with variance explained metrics\"\"\"\n    X_centered = X - X.mean(axis=1, keepdims=True)\n    C = X_centered @ X_centered.T / X.shape[1]\n    eigvals, eigvecs = np.linalg.eigh(C)\n    idx = eigvals.argsort()[::-1]\n\n    V = eigvecs[:, idx][:, :s]\n    Z = V.T @ X_centered\n\n    metrics = {\n        \"explained_variance\": eigvals[idx][:s].tolist(),\n        \"total_variance_ratio\": eigvals[idx][:s].sum() / eigvals.sum(),\n    }\n    return Z, metrics\n\n\ndef kernel_matrix(\n    X: np.ndarray, kernel: str, gamma: float = 1.0, degree: int = 2\n) -&gt; np.ndarray:\n    \"\"\"Compute centered kernel matrix with RBF/poly/linear kernels\"\"\"\n    n = X.shape[1]\n    K = np.zeros((n, n))\n\n    # Kernel computations\n    for i in range(n):\n        for j in range(n):\n            if kernel == \"rbf\":\n                K[i, j] = np.exp(-gamma * np.linalg.norm(X[:, i] - X[:, j]) ** 2)\n            elif kernel == \"poly\":\n                K[i, j] = (X[:, i] @ X[:, j] + 1) ** degree\n            else:  # linear kernel\n                K[i, j] = X[:, i] @ X[:, j]\n\n    # Center kernel matrix\n    J = np.ones((n, n)) / n\n    K_centered = K - J @ K - K @ J + J @ K @ J\n    return K_centered\n\n\ndef compute_kpca(\n    X: np.ndarray, kernel: str, s: int, **kernel_params\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"Kernel PCA implementation with automatic centering\"\"\"\n    K = kernel_matrix(X, kernel, **kernel_params)\n    eigvals, eigvecs = np.linalg.eigh(K)\n    idx = eigvals.argsort()[::-1]\n\n    alpha = eigvecs[:, idx][:, :s]\n    for i in range(s):\n        alpha[:, i] /= np.sqrt(eigvals[idx][i])  # Normalize eigenvectors\n\n    Z = alpha.T @ K\n    metrics = {\n        \"top_eigenvalues\": eigvals[idx][:s].tolist(),\n        \"kernel_matrix_sample\": K[:3, :3].tolist(),\n    }\n    return Z, metrics\n\n\n\n\nCode\ndef create_projection_plot(Z: np.ndarray, labels: np.ndarray) -&gt; go.Figure:\n    \"\"\"Create interactive 3D/2D visualization of projected data\"\"\"\n    dim = Z.shape[0]\n    if dim == 1:\n        Z = np.vstack([Z, np.zeros_like(Z)])\n\n    df = {\n        \"PC1\": Z[0],\n        \"PC2\": Z[1],\n        \"PC3\": Z[2] if dim &gt; 2 else np.zeros_like(Z[0]),\n        \"Class\": labels.astype(str),\n    }\n\n    fig = (\n        px.scatter_3d(df, x=\"PC1\", y=\"PC2\", z=\"PC3\", color=\"Class\")\n        if dim &gt; 2\n        else px.scatter(df, x=\"PC1\", y=\"PC2\", color=\"Class\")\n    )\n\n    fig.update_layout(height=600, scene_camera=dict(up=dict(x=0, y=0, z=1)))\n    return fig"
  },
  {
    "objectID": "content/05-dimension-reduction.html#interactive-dashboard",
    "href": "content/05-dimension-reduction.html#interactive-dashboard",
    "title": "Dimension Reduction",
    "section": "5 Interactive Dashboard",
    "text": "5 Interactive Dashboard\n\n\nCode\ndataset_map = {\n    \"Breast Cancer\": load_breast_cancer,\n    \"Digits\": load_digits,\n    \"Iris\": load_iris,\n    \"Wine\": load_wine,\n}\ndataset_names = sorted(list(dataset_map.keys()))\n\n\n\n\nCode\ndef analyze_data(\n    dataset_name: str,\n    method: str,\n    n_components: int,\n    kernel: str,\n    gamma: float,\n    degree: int,\n) -&gt; tuple[go.Figure, dict]:\n    \"\"\"Main analysis function handling dataset loading and processing\"\"\"\n    # Load dataset\n    data = dataset_map[dataset_name]()\n    X = StandardScaler().fit_transform(data.data).T\n    labels = data.target\n\n    # Compute projections\n    if method == \"PCA\":\n        Z, metrics = compute_pca(X, n_components)\n        metric_key = \"PCA Metrics\"\n    else:\n        Z, metrics = compute_kpca(X, kernel, n_components, gamma=gamma, degree=degree)\n        metric_key = \"Kernel PCA Metrics\"\n\n    # Generate visualization\n    fig = create_projection_plot(Z, labels)\n    return fig, {metric_key: metrics}\n\n\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"Dimension Reduction Explorer\",\n) as demo:\n    with gr.Row():\n        dataset = gr.Dropdown(dataset_names, label=\"Dataset\", value=dataset_names[0])\n        method = gr.Radio([\"PCA\", \"Kernel PCA\"], label=\"Method\", value=\"PCA\")\n        components = gr.Slider(1, 3, value=3, step=1, label=\"Components\")\n\n    with gr.Accordion(\"Kernel Parameters\", open=False):\n        kernel = gr.Dropdown([\"linear\", \"rbf\", \"poly\"], value=\"linear\", label=\"Kernel\")\n        gamma = gr.Slider(0.01, 1.0, value=0.005, label=\"RBF Gamma\", visible=False)\n        degree = gr.Slider(\n            1, 5, value=2, step=1, label=\"Polynomial Degree\", visible=False\n        )\n\n    analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n\n    with gr.Tabs():\n        with gr.Tab(\"Projection\"):\n            plot = gr.Plot(label=\"Data Projection\")\n        with gr.Tab(\"Metrics\"):\n            metrics = gr.JSON(label=\"Analysis Metrics\")\n\n    kernel.change(\n        lambda k: (gr.update(visible=k == \"rbf\"), gr.update(visible=k == \"poly\")),\n        inputs=kernel,\n        outputs=[gamma, degree],\n    )\n\n    analyze_btn.click(\n        analyze_data,\n        inputs=[dataset, method, components, kernel, gamma, degree],\n        outputs=[plot, metrics],\n    )\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport gradio as gr\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.datasets import load_breast_cancer, load_digits, load_iris, load_wine\nfrom sklearn.preprocessing import StandardScaler\ndef svd_low_rank_approximation(A: np.ndarray, p: int) -&gt; np.ndarray:\n    \"\"\"Compute pseudo-inverse using low-rank SVD approximation\"\"\"\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    S_inv = np.diag(1 / s[:p])\n    return Vt[:p].T @ S_inv @ U[:, :p].T\n\n\ndef compute_pca(X: np.ndarray, s: int) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"PCA implementation with variance explained metrics\"\"\"\n    X_centered = X - X.mean(axis=1, keepdims=True)\n    C = X_centered @ X_centered.T / X.shape[1]\n    eigvals, eigvecs = np.linalg.eigh(C)\n    idx = eigvals.argsort()[::-1]\n\n    V = eigvecs[:, idx][:, :s]\n    Z = V.T @ X_centered\n\n    metrics = {\n        \"explained_variance\": eigvals[idx][:s].tolist(),\n        \"total_variance_ratio\": eigvals[idx][:s].sum() / eigvals.sum(),\n    }\n    return Z, metrics\n\n\ndef kernel_matrix(\n    X: np.ndarray, kernel: str, gamma: float = 1.0, degree: int = 2\n) -&gt; np.ndarray:\n    \"\"\"Compute centered kernel matrix with RBF/poly/linear kernels\"\"\"\n    n = X.shape[1]\n    K = np.zeros((n, n))\n\n    # Kernel computations\n    for i in range(n):\n        for j in range(n):\n            if kernel == \"rbf\":\n                K[i, j] = np.exp(-gamma * np.linalg.norm(X[:, i] - X[:, j]) ** 2)\n            elif kernel == \"poly\":\n                K[i, j] = (X[:, i] @ X[:, j] + 1) ** degree\n            else:  # linear kernel\n                K[i, j] = X[:, i] @ X[:, j]\n\n    # Center kernel matrix\n    J = np.ones((n, n)) / n\n    K_centered = K - J @ K - K @ J + J @ K @ J\n    return K_centered\n\n\ndef compute_kpca(\n    X: np.ndarray, kernel: str, s: int, **kernel_params\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"Kernel PCA implementation with automatic centering\"\"\"\n    K = kernel_matrix(X, kernel, **kernel_params)\n    eigvals, eigvecs = np.linalg.eigh(K)\n    idx = eigvals.argsort()[::-1]\n\n    alpha = eigvecs[:, idx][:, :s]\n    for i in range(s):\n        alpha[:, i] /= np.sqrt(eigvals[idx][i])  # Normalize eigenvectors\n\n    Z = alpha.T @ K\n    metrics = {\n        \"top_eigenvalues\": eigvals[idx][:s].tolist(),\n        \"kernel_matrix_sample\": K[:3, :3].tolist(),\n    }\n    return Z, metrics\ndef create_projection_plot(Z: np.ndarray, labels: np.ndarray) -&gt; go.Figure:\n    \"\"\"Create interactive 3D/2D visualization of projected data\"\"\"\n    dim = Z.shape[0]\n    if dim == 1:\n        Z = np.vstack([Z, np.zeros_like(Z)])\n\n    df = {\n        \"PC1\": Z[0],\n        \"PC2\": Z[1],\n        \"PC3\": Z[2] if dim &gt; 2 else np.zeros_like(Z[0]),\n        \"Class\": labels.astype(str),\n    }\n\n    fig = (\n        px.scatter_3d(df, x=\"PC1\", y=\"PC2\", z=\"PC3\", color=\"Class\")\n        if dim &gt; 2\n        else px.scatter(df, x=\"PC1\", y=\"PC2\", color=\"Class\")\n    )\n\n    fig.update_layout(height=600, scene_camera=dict(up=dict(x=0, y=0, z=1)))\n    return fig\ndataset_map = {\n    \"Breast Cancer\": load_breast_cancer,\n    \"Digits\": load_digits,\n    \"Iris\": load_iris,\n    \"Wine\": load_wine,\n}\ndataset_names = sorted(list(dataset_map.keys()))\ndef analyze_data(\n    dataset_name: str,\n    method: str,\n    n_components: int,\n    kernel: str,\n    gamma: float,\n    degree: int,\n) -&gt; tuple[go.Figure, dict]:\n    \"\"\"Main analysis function handling dataset loading and processing\"\"\"\n    # Load dataset\n    data = dataset_map[dataset_name]()\n    X = StandardScaler().fit_transform(data.data).T\n    labels = data.target\n\n    # Compute projections\n    if method == \"PCA\":\n        Z, metrics = compute_pca(X, n_components)\n        metric_key = \"PCA Metrics\"\n    else:\n        Z, metrics = compute_kpca(X, kernel, n_components, gamma=gamma, degree=degree)\n        metric_key = \"Kernel PCA Metrics\"\n\n    # Generate visualization\n    fig = create_projection_plot(Z, labels)\n    return fig, {metric_key: metrics}\n\n\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"Dimension Reduction Explorer\",\n) as demo:\n    with gr.Row():\n        dataset = gr.Dropdown(dataset_names, label=\"Dataset\", value=dataset_names[0])\n        method = gr.Radio([\"PCA\", \"Kernel PCA\"], label=\"Method\", value=\"PCA\")\n        components = gr.Slider(1, 3, value=3, step=1, label=\"Components\")\n\n    with gr.Accordion(\"Kernel Parameters\", open=False):\n        kernel = gr.Dropdown([\"linear\", \"rbf\", \"poly\"], value=\"linear\", label=\"Kernel\")\n        gamma = gr.Slider(0.01, 1.0, value=0.005, label=\"RBF Gamma\", visible=False)\n        degree = gr.Slider(\n            1, 5, value=2, step=1, label=\"Polynomial Degree\", visible=False\n        )\n\n    analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n\n    with gr.Tabs():\n        with gr.Tab(\"Projection\"):\n            plot = gr.Plot(label=\"Data Projection\")\n        with gr.Tab(\"Metrics\"):\n            metrics = gr.JSON(label=\"Analysis Metrics\")\n\n    kernel.change(\n        lambda k: (gr.update(visible=k == \"rbf\"), gr.update(visible=k == \"poly\")),\n        inputs=kernel,\n        outputs=[gamma, degree],\n    )\n\n    analyze_btn.click(\n        analyze_data,\n        inputs=[dataset, method, components, kernel, gamma, degree],\n        outputs=[plot, metrics],\n    )\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/01-machine-numbers.html",
    "href": "content/01-machine-numbers.html",
    "title": "Machine Numbers",
    "section": "",
    "text": "Code\nimport math\nimport struct\nfrom fractions import Fraction\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gradio as gr"
  },
  {
    "objectID": "content/01-machine-numbers.html#introduction",
    "href": "content/01-machine-numbers.html#introduction",
    "title": "Machine Numbers",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis notebook is crafted to provide insights into how computers represent numbers, with a specific focus on three major types:\n\nIntegers: We cover the representation of 64-bit signed integers and delve into the concept of overflow, where values exceed the maximum representable integer.\nRationals: Introduce rational numbers, conceptualized using fractions derived from 64-bit integers. Discuss encoding concepts for special cases like infinity.\nFloating Point Numbers: A detailed look at the IEEE 754 double precision (64-bit) standard, covering:\n\nThe structure: Composed of 1 bit for the sign, 11 bits for the exponent, and 52 bits for the mantissa.\nThe representation format:\n\\[\n\\pm (1,\\,a_1 a_2\\ldots a_{52})_2 \\cdot 2^\\alpha = (-1)^s 2^\\alpha\\left(1+\\sum_{k=1}^{52} a_k 2^{-k}\\right),\n\\]\nwhere \\(s,\\,a_k\\in\\{0,1\\}\\) and exponent \\(\\alpha\\in\\{-1022,\\ldots,1023\\}\\) with special values for \\(-1023\\) and \\(1024\\).\nThe concept of machine precision:\n\\[\n\\mathrm{eps}_{64} = 2^{-52}\\approx 2.220446049250313 \\times 10^{-16},\n\\]\ndefining the bounds of relative error when rounding real numbers.\nSubnormal numbers: These provide extended range of representation near zero:\n\\[\n2^{-1074}.\n\\]"
  },
  {
    "objectID": "content/01-machine-numbers.html#python-implementation",
    "href": "content/01-machine-numbers.html#python-implementation",
    "title": "Machine Numbers",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\n\n2.1 Integer Representation\nUnderstanding the binary representation of integers is foundational. We represent 64-bit signed integers using NumPyâ€™s np.int64. Binary representation is obtained using twoâ€™s complement method. The code below defines int64_bitstring, which outputs the 64-bit binary equivalent of an integer:\n\n\nCode\ndef int64_bitstring(n: int) -&gt; str:\n    \"\"\"\n    Return the 64-bit two's complement binary representation of an integer.\n\n    Args:\n      n (int): The integer (should be in the range of 64-bit signed integers).\n\n    Returns:\n      str: A string of 64 characters ('0' or '1') representing the number.\n    \"\"\"\n    # Convert to numpy.int64 to simulate 64-bit behavior\n    n64 = np.int64(n)\n    # np.binary_repr works for negative numbers if width is provided.\n    return np.binary_repr(n64, width=64)\n\n\n\n\n2.2 Rational Numbers\nRational numbers can be depicted with Pythonâ€™s Fraction class. This handles fractions using pairs of integers. A special consideration is made for division by zero, where we conventionally define:\n\nA zero denominator results in \\(\\infty\\) or \\(-\\infty\\) based on the numeratorâ€™s sign.\n\n\n\nCode\ndef rational(p: int, q: int) -&gt; Fraction | float:\n    \"\"\"\n    Create a rational number from two integers.\n\n    If q == 0, return positive or negative infinity based on the sign of p.\n\n    Args:\n      p (int): Numerator.\n      q (int): Denominator.\n\n    Returns:\n      Fraction or float: A Fraction instance if q != 0, or math.inf/-math.inf if q == 0.\n    \"\"\"\n    if q == 0:\n        return math.inf if p &gt;= 0 else -math.inf\n    return Fraction(p, q)\n\n\n\n\n2.3 Floating Point Number Representation\nFloating point numbers follow the IEEE 754 format. This section highlights code implementation for extracting components of a double-precision float:\nThe function float_to_bitstring unpacks a float into:\n\nA complete 64-bit binary string.\nSign, exponent, and mantissa breakdown.\n\nThe layout consists of:\n\nBit 63: Sign\nBits 62â€“52: Exponent\nBits 51â€“0: Mantissa\n\n\n\nCode\ndef float_to_bitstring(x: float) -&gt; dict:\n    \"\"\"\n    Convert a Python float into its IEEE 754 double precision bit components.\n\n    Args:\n      x (float): The floating point number.\n\n    Returns:\n      dict: A dictionary with keys 'full', 'sign', 'exponent', 'mantissa'.\n    \"\"\"\n    # Ensure x is a numpy.float64 to avoid precision issues\n    x = np.float64(x)\n    # Pack float into 8 bytes and unpack as unsigned long long (64-bit integer)\n    packed = struct.pack(\"!d\", x)\n    (bits,) = struct.unpack(\"!Q\", packed)\n    full = format(bits, \"064b\")\n    sign = full[0]\n    exponent_bits = full[1:12]\n    mantissa_bits = full[12:]\n    exponent_val = int(exponent_bits, 2)\n    mantissa_val = int(mantissa_bits, 2)\n    return {\n        \"full\": full,\n        \"sign\": sign,\n        \"exponent\": exponent_val,\n        \"mantissa\": mantissa_val,\n    }\n\n\n\n\n2.4 Machine Precision and Epsilon\nMachine precision, or epsilon (\\(\\epsilon_{64}\\)), is crucial for understanding floating point accuracy:\n\\[\n\\epsilon_{64} = 2^{-52}.\n\\]\nHereâ€™s the code that returns machine epsilon directly and via simulation:\n\n\nCode\ndef machine_epsilon() -&gt; np.float64:\n    \"\"\"\n    Return the IEEE 754 double precision machine epsilon, defined as 2^-52.\n\n    Returns:\n      np.float64: The machine precision.\n    \"\"\"\n    return np.float64(2.0) ** np.float64(-52)\n\n\ndef simulate_myeps() -&gt; np.float64:\n    \"\"\"\n    Simulate the determination of machine epsilon by iterative halving.\n\n    Returns:\n      np.float64: The computed machine epsilon.\n    \"\"\"\n    eps = np.float64(1.0)\n    while np.float64(1.0) + eps &gt; np.float64(1.0):\n        prev = eps\n        eps /= np.float64(2.0)\n    return prev\n\n\n\n\n2.5 Next Representable Float\nThe concept of the next representable float demonstrates how floating point precision works. next_float, facilitated by NumPyâ€™s nextafter, finds the subsequent float in a specified direction:\n\n\nCode\ndef next_float(x: float, direction: float) -&gt; np.float64:\n    \"\"\"\n    Get the next representable floating point number after x in the direction of 'direction'.\n\n    Args:\n      x (float): Starting floating point number.\n      direction (float): The direction (e.g., 1.0 for upward, -1.0 for downward).\n\n    Returns:\n      np.float64: The next representable floating point number.\n    \"\"\"\n    # Convert inputs to np.float64 to ensure correct behavior.\n    return np.nextafter(np.float64(x), np.float64(direction))\n\n\n\n\n2.6 Plotting the Distribution of Floating Point Gaps\nBetween any two consecutive normalized floating point numbers with exponent \\(\\alpha\\), the gap size is:\n\\[\n\\text{gap} = 2^{\\alpha-52}.\n\\]\nThe plot_vlines function visualizes these gap sizes across different exponents by plotting vertical lines:\n\n\nCode\ndef plot_vlines(n: int) -&gt; plt.Figure:\n    \"\"\"\n    Plot vertical lines at positions f(alpha)=2^alpha for alpha from -1022 up to n.\n\n    Args:\n      n (int): The maximum exponent (n must be between -1021 and 1023).\n\n    Returns:\n      plt.Figure: The generated matplotlib figure.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(8, 6))\n    x_vals = [2.0**i for i in range(-1022, n + 1)]\n    for x in x_vals:\n        ax.axvline(x=x, color=\"blue\")\n    ax.set_xlim([0, 2.0**n])\n    ax.set_ylim([0, 1])\n    ax.set_title(r\"there are $2^{52} - 1$ Float64 numbers between two lines\")\n    ax.set_xlabel(r\"$2^\\alpha$, where $\\alpha = -1022, \\dots,\" + f\"{n}$\")\n    plt.close(fig)  # Prevents immediate display in non-interactive environments\n    return fig"
  },
  {
    "objectID": "content/01-machine-numbers.html#interactive-dashboard",
    "href": "content/01-machine-numbers.html#interactive-dashboard",
    "title": "Machine Numbers",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\nThe interactive dashboard enables exploration and visualization through the following tabs:\n\nInteger Representation:\nEnter an integer to observe its 64-bit binary (twoâ€™s complement) representation.\nRational Demonstration:\nProvide a numerator and a denominator to see the simplified rational output or infinity if applicable.\nFloating Point Representation:\nInput a float to obtain its detailed binary structure, including sign, exponent, and mantissa.\nMachine Epsilon and Subnormals:\nDiscover machine epsilonâ€™s direct and simulated values, including the smallest positive subnormal.\nGap Distribution Plot:\nAdjust the exponent range to visualize the gap sizes between consecutive floating point numbers.\n\n\n\nCode\ndef gradio_int_representation(n: int) -&gt; dict:\n    \"\"\"\n    Gradio interface function to return the 64-bit representation of an integer.\n\n    Args:\n      n (int): The input integer.\n\n    Returns:\n      dict: A dictionary containing the original integer and its 64-bit binary string.\n    \"\"\"\n    return {\"input_integer\": n, \"64-bit_representation\": int64_bitstring(n)}\n\n\ndef gradio_rational(p: int, q: int) -&gt; dict:\n    \"\"\"\n    Gradio interface function for rational numbers.\n\n    Args:\n      p (int): Numerator.\n      q (int): Denominator.\n\n    Returns:\n      dict: A dictionary with the input, simplified fraction (if possible),\n            or an indication of infinity when q == 0.\n    \"\"\"\n    result = rational(p, q)\n    if isinstance(result, Fraction):\n        simplified = result  # Fraction automatically simplifies.\n    else:\n        simplified = result\n    return {\"numerator\": p, \"denominator\": q, \"result\": str(simplified)}\n\n\ndef gradio_float_representation(x: float) -&gt; dict:\n    \"\"\"\n    Gradio interface function for floating point representation.\n\n    Args:\n      x (float): The input floating point number.\n\n    Returns:\n      dict: A dictionary containing the float's IEEE 754 components.\n    \"\"\"\n    comp = float_to_bitstring(x)\n    return {\n        \"input_float\": x,\n        \"IEEE754_full\": comp[\"full\"],\n        \"Sign_bit\": comp[\"sign\"],\n        \"Exponent_field (as integer)\": comp[\"exponent\"],\n        \"Mantissa_field (as integer)\": comp[\"mantissa\"],\n    }\n\n\ndef gradio_machine_epsilon() -&gt; dict:\n    \"\"\"\n    Gradio interface function to display machine epsilon and next float after 0.\n\n    Returns:\n      dict: A dictionary with the direct machine epsilon, simulated epsilon,\n            and the smallest positive subnormal (next float after 0).\n    \"\"\"\n    eps_direct = float(machine_epsilon())\n    eps_sim = float(simulate_myeps())\n    next_after_zero = float(next_float(0.0, 1.0))\n    return {\n        \"machine_epsilon (2^-52)\": eps_direct,\n        \"simulated_machine_epsilon\": eps_sim,\n        \"next_float(0.0, 1.0) [smallest positive subnormal]\": next_after_zero,\n    }\n\n\ndef gradio_gap_distribution(n: int) -&gt; gr.Plot:\n    \"\"\"\n    Gradio interface function to produce a plot with vertical lines.\n\n    Args:\n      n (int): The maximum exponent for which to plot the vertical lines.\n\n    Returns:\n      gr.Plot: A Gradio-compatible plot generated by plot_vlines.\n    \"\"\"\n    fig = plot_vlines(n)\n    return fig\n\n\nBelow, we build the Gradio interface with multiple tabs to interact with the different demonstrations.\n\n\nCode\nwith gr.Blocks(css=\"\"\"gradio-app {background: #222222 !important}\"\"\") as demo:\n    gr.Markdown(\"# IEEE 754 and Number Representations\")\n\n    with gr.Tabs():\n        with gr.TabItem(\"Gap Distribution Plot\"):\n            gr.Markdown(\"### Distribution of Gap Sizes in Floating Point Numbers\")\n            n_slider = gr.Slider(\n                label=\"Max Exponent\",\n                value=-1021,\n                interactive=True,\n                minimum=-1021,\n                maximum=1023,\n                step=1,\n            )\n            plot_output = gr.Plot(label=\"Gap Distribution\")\n            plot_button = gr.Button(\"Plot Gap Distribution\")\n            plot_button.click(\n                fn=gradio_gap_distribution, inputs=n_slider, outputs=plot_output\n            )\n\n        with gr.TabItem(\"Integer Representation\"):\n            gr.Markdown(\"### 64-bit Integer Representation\")\n            int_input = gr.Number(label=\"Enter an integer\", value=42, precision=0)\n            int_output = gr.JSON(label=\"64-bit Representation\")\n            int_button = gr.Button(\"Show Bitstring\")\n            int_button.click(\n                fn=gradio_int_representation, inputs=int_input, outputs=int_output\n            )\n\n        with gr.TabItem(\"Rational Demonstration\"):\n            gr.Markdown(\"### Rational Numbers\")\n            p_input = gr.Number(label=\"Numerator (p)\", value=2, precision=0)\n            q_input = gr.Number(label=\"Denominator (q)\", value=3, precision=0)\n            rat_output = gr.JSON(label=\"Rational Output\")\n            rat_button = gr.Button(\"Compute Rational\")\n            rat_button.click(\n                fn=gradio_rational, inputs=[p_input, q_input], outputs=rat_output\n            )\n\n        with gr.TabItem(\"Floating Point Representation\"):\n            gr.Markdown(\"### IEEE 754 Double Precision Breakdown\")\n            float_input = gr.Number(\n                label=\"Enter a floating point number\", value=3.14159\n            )\n            float_output = gr.JSON(label=\"IEEE 754 Components\")\n            float_button = gr.Button(\"Show Floating Point Bits\")\n            float_button.click(\n                fn=gradio_float_representation, inputs=float_input, outputs=float_output\n            )\n\n        with gr.TabItem(\"Machine Epsilon and Subnormals\"):\n            gr.Markdown(\"### Machine Precision and Next Float\")\n            eps_output = gr.JSON(label=\"Machine Epsilon and Next Float\")\n            eps_button = gr.Button(\"Show Machine Epsilon and Next Float\")\n            eps_button.click(fn=gradio_machine_epsilon, inputs=[], outputs=eps_output)\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport math\nimport struct\nfrom fractions import Fraction\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gradio as gr\ndef int64_bitstring(n: int) -&gt; str:\n    \"\"\"\n    Return the 64-bit two's complement binary representation of an integer.\n\n    Args:\n      n (int): The integer (should be in the range of 64-bit signed integers).\n\n    Returns:\n      str: A string of 64 characters ('0' or '1') representing the number.\n    \"\"\"\n    # Convert to numpy.int64 to simulate 64-bit behavior\n    n64 = np.int64(n)\n    # np.binary_repr works for negative numbers if width is provided.\n    return np.binary_repr(n64, width=64)\ndef rational(p: int, q: int) -&gt; Fraction | float:\n    \"\"\"\n    Create a rational number from two integers.\n\n    If q == 0, return positive or negative infinity based on the sign of p.\n\n    Args:\n      p (int): Numerator.\n      q (int): Denominator.\n\n    Returns:\n      Fraction or float: A Fraction instance if q != 0, or math.inf/-math.inf if q == 0.\n    \"\"\"\n    if q == 0:\n        return math.inf if p &gt;= 0 else -math.inf\n    return Fraction(p, q)\ndef float_to_bitstring(x: float) -&gt; dict:\n    \"\"\"\n    Convert a Python float into its IEEE 754 double precision bit components.\n\n    Args:\n      x (float): The floating point number.\n\n    Returns:\n      dict: A dictionary with keys 'full', 'sign', 'exponent', 'mantissa'.\n    \"\"\"\n    # Ensure x is a numpy.float64 to avoid precision issues\n    x = np.float64(x)\n    # Pack float into 8 bytes and unpack as unsigned long long (64-bit integer)\n    packed = struct.pack(\"!d\", x)\n    (bits,) = struct.unpack(\"!Q\", packed)\n    full = format(bits, \"064b\")\n    sign = full[0]\n    exponent_bits = full[1:12]\n    mantissa_bits = full[12:]\n    exponent_val = int(exponent_bits, 2)\n    mantissa_val = int(mantissa_bits, 2)\n    return {\n        \"full\": full,\n        \"sign\": sign,\n        \"exponent\": exponent_val,\n        \"mantissa\": mantissa_val,\n    }\ndef machine_epsilon() -&gt; np.float64:\n    \"\"\"\n    Return the IEEE 754 double precision machine epsilon, defined as 2^-52.\n\n    Returns:\n      np.float64: The machine precision.\n    \"\"\"\n    return np.float64(2.0) ** np.float64(-52)\n\n\ndef simulate_myeps() -&gt; np.float64:\n    \"\"\"\n    Simulate the determination of machine epsilon by iterative halving.\n\n    Returns:\n      np.float64: The computed machine epsilon.\n    \"\"\"\n    eps = np.float64(1.0)\n    while np.float64(1.0) + eps &gt; np.float64(1.0):\n        prev = eps\n        eps /= np.float64(2.0)\n    return prev\ndef next_float(x: float, direction: float) -&gt; np.float64:\n    \"\"\"\n    Get the next representable floating point number after x in the direction of 'direction'.\n\n    Args:\n      x (float): Starting floating point number.\n      direction (float): The direction (e.g., 1.0 for upward, -1.0 for downward).\n\n    Returns:\n      np.float64: The next representable floating point number.\n    \"\"\"\n    # Convert inputs to np.float64 to ensure correct behavior.\n    return np.nextafter(np.float64(x), np.float64(direction))\ndef plot_vlines(n: int) -&gt; plt.Figure:\n    \"\"\"\n    Plot vertical lines at positions f(alpha)=2^alpha for alpha from -1022 up to n.\n\n    Args:\n      n (int): The maximum exponent (n must be between -1021 and 1023).\n\n    Returns:\n      plt.Figure: The generated matplotlib figure.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(8, 6))\n    x_vals = [2.0**i for i in range(-1022, n + 1)]\n    for x in x_vals:\n        ax.axvline(x=x, color=\"blue\")\n    ax.set_xlim([0, 2.0**n])\n    ax.set_ylim([0, 1])\n    ax.set_title(r\"there are $2^{52} - 1$ Float64 numbers between two lines\")\n    ax.set_xlabel(r\"$2^\\alpha$, where $\\alpha = -1022, \\dots,\" + f\"{n}$\")\n    plt.close(fig)  # Prevents immediate display in non-interactive environments\n    return fig\ndef gradio_int_representation(n: int) -&gt; dict:\n    \"\"\"\n    Gradio interface function to return the 64-bit representation of an integer.\n\n    Args:\n      n (int): The input integer.\n\n    Returns:\n      dict: A dictionary containing the original integer and its 64-bit binary string.\n    \"\"\"\n    return {\"input_integer\": n, \"64-bit_representation\": int64_bitstring(n)}\n\n\ndef gradio_rational(p: int, q: int) -&gt; dict:\n    \"\"\"\n    Gradio interface function for rational numbers.\n\n    Args:\n      p (int): Numerator.\n      q (int): Denominator.\n\n    Returns:\n      dict: A dictionary with the input, simplified fraction (if possible),\n            or an indication of infinity when q == 0.\n    \"\"\"\n    result = rational(p, q)\n    if isinstance(result, Fraction):\n        simplified = result  # Fraction automatically simplifies.\n    else:\n        simplified = result\n    return {\"numerator\": p, \"denominator\": q, \"result\": str(simplified)}\n\n\ndef gradio_float_representation(x: float) -&gt; dict:\n    \"\"\"\n    Gradio interface function for floating point representation.\n\n    Args:\n      x (float): The input floating point number.\n\n    Returns:\n      dict: A dictionary containing the float's IEEE 754 components.\n    \"\"\"\n    comp = float_to_bitstring(x)\n    return {\n        \"input_float\": x,\n        \"IEEE754_full\": comp[\"full\"],\n        \"Sign_bit\": comp[\"sign\"],\n        \"Exponent_field (as integer)\": comp[\"exponent\"],\n        \"Mantissa_field (as integer)\": comp[\"mantissa\"],\n    }\n\n\ndef gradio_machine_epsilon() -&gt; dict:\n    \"\"\"\n    Gradio interface function to display machine epsilon and next float after 0.\n\n    Returns:\n      dict: A dictionary with the direct machine epsilon, simulated epsilon,\n            and the smallest positive subnormal (next float after 0).\n    \"\"\"\n    eps_direct = float(machine_epsilon())\n    eps_sim = float(simulate_myeps())\n    next_after_zero = float(next_float(0.0, 1.0))\n    return {\n        \"machine_epsilon (2^-52)\": eps_direct,\n        \"simulated_machine_epsilon\": eps_sim,\n        \"next_float(0.0, 1.0) [smallest positive subnormal]\": next_after_zero,\n    }\n\n\ndef gradio_gap_distribution(n: int) -&gt; gr.Plot:\n    \"\"\"\n    Gradio interface function to produce a plot with vertical lines.\n\n    Args:\n      n (int): The maximum exponent for which to plot the vertical lines.\n\n    Returns:\n      gr.Plot: A Gradio-compatible plot generated by plot_vlines.\n    \"\"\"\n    fig = plot_vlines(n)\n    return fig\nwith gr.Blocks(css=\"\"\"gradio-app {background: #222222 !important}\"\"\") as demo:\n    gr.Markdown(\"# IEEE 754 and Number Representations\")\n\n    with gr.Tabs():\n        with gr.TabItem(\"Gap Distribution Plot\"):\n            gr.Markdown(\"### Distribution of Gap Sizes in Floating Point Numbers\")\n            n_slider = gr.Slider(\n                label=\"Max Exponent\",\n                value=-1021,\n                interactive=True,\n                minimum=-1021,\n                maximum=1023,\n                step=1,\n            )\n            plot_output = gr.Plot(label=\"Gap Distribution\")\n            plot_button = gr.Button(\"Plot Gap Distribution\")\n            plot_button.click(\n                fn=gradio_gap_distribution, inputs=n_slider, outputs=plot_output\n            )\n\n        with gr.TabItem(\"Integer Representation\"):\n            gr.Markdown(\"### 64-bit Integer Representation\")\n            int_input = gr.Number(label=\"Enter an integer\", value=42, precision=0)\n            int_output = gr.JSON(label=\"64-bit Representation\")\n            int_button = gr.Button(\"Show Bitstring\")\n            int_button.click(\n                fn=gradio_int_representation, inputs=int_input, outputs=int_output\n            )\n\n        with gr.TabItem(\"Rational Demonstration\"):\n            gr.Markdown(\"### Rational Numbers\")\n            p_input = gr.Number(label=\"Numerator (p)\", value=2, precision=0)\n            q_input = gr.Number(label=\"Denominator (q)\", value=3, precision=0)\n            rat_output = gr.JSON(label=\"Rational Output\")\n            rat_button = gr.Button(\"Compute Rational\")\n            rat_button.click(\n                fn=gradio_rational, inputs=[p_input, q_input], outputs=rat_output\n            )\n\n        with gr.TabItem(\"Floating Point Representation\"):\n            gr.Markdown(\"### IEEE 754 Double Precision Breakdown\")\n            float_input = gr.Number(\n                label=\"Enter a floating point number\", value=3.14159\n            )\n            float_output = gr.JSON(label=\"IEEE 754 Components\")\n            float_button = gr.Button(\"Show Floating Point Bits\")\n            float_button.click(\n                fn=gradio_float_representation, inputs=float_input, outputs=float_output\n            )\n\n        with gr.TabItem(\"Machine Epsilon and Subnormals\"):\n            gr.Markdown(\"### Machine Precision and Next Float\")\n            eps_output = gr.JSON(label=\"Machine Epsilon and Next Float\")\n            eps_button = gr.Button(\"Show Machine Epsilon and Next Float\")\n            eps_button.click(fn=gradio_machine_epsilon, inputs=[], outputs=eps_output)\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/06-clustering---svm.html",
    "href": "content/06-clustering---svm.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Code\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nimport gradio as gr"
  },
  {
    "objectID": "content/06-clustering---svm.html#introduction",
    "href": "content/06-clustering---svm.html#introduction",
    "title": "Support Vector Machines",
    "section": "1 Introduction",
    "text": "1 Introduction\nSupport Vector Machines (SVM) are a class of supervised learning models used for classification and regression analysis. SVMs are particularly effective for high-dimensional spaces and are versatile due to their ability to use kernel functions for decision boundary clarity. This notebook explores SVMs using Scikit-learnâ€™s SVC for binary classification tasks with different kernel functions.\n\n1.1 SVM Decision Function\nGiven a set of training data \\((x_i, y_i)\\) with \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{-1,1\\}\\), the decision function is:\n\\[\nf(x) = \\operatorname{sgn}\\left(\\sum_{i=1}^{n} \\alpha_i\\, k(x,x_i) + b\\right),\n\\]\nwhere \\(k(x,x_i)\\) is a kernel function (e.g., Gaussian, Polynomial, or Linear). SVM seeks the hyperplane that maximizes the margin between the two classes. We will visualize the decision boundary and highlight the support vectors in this notebook."
  },
  {
    "objectID": "content/06-clustering---svm.html#python-implementation",
    "href": "content/06-clustering---svm.html#python-implementation",
    "title": "Support Vector Machines",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\n\n\nCode\ndef load_dataset(name: str) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Load a dataset from scikit-learn.\n\n    Parameters:\n        name (str): Name of the dataset ('Breast Cancer', 'Digits', 'Iris', 'Wine').\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: Feature matrix X and label vector y.\n    \"\"\"\n    if name == \"Breast Cancer\":\n        data = datasets.load_breast_cancer()\n    elif name == \"Digits\":\n        data = datasets.load_digits()\n    elif name == \"Iris\":\n        data = datasets.load_iris()\n    elif name == \"Wine\":\n        data = datasets.load_wine()\n    else:\n        raise ValueError(\"Dataset not supported.\")\n\n    X = data.data\n    y = data.target\n    # For multi-class datasets, restrict to the first two classes for binary classification.\n    if len(np.unique(y)) &gt; 2:\n        mask = y &lt; 2\n        X = X[mask]\n        y = y[mask]\n    # Map labels: 0 -&gt; -1, 1 -&gt; 1\n    y = np.where(y == 0, -1, 1)\n    return X, y\n\n\ndef reduce_dimensionality(X: np.ndarray, n_components: int = 2) -&gt; np.ndarray:\n    \"\"\"\n    Reduce dimensionality of X using PCA.\n\n    Parameters:\n        X (np.ndarray): Feature matrix.\n        n_components (int): Number of principal components.\n\n    Returns:\n        np.ndarray: Data reduced to n_components dimensions.\n    \"\"\"\n    pca = PCA(n_components=n_components)\n    return pca.fit_transform(X)\n\n\nWe define a function to create a Plotly contour plot representing the decision function of the classifier.\nIt overlays the decision boundary, the training points, and highlights the support vectors.\n\n\nCode\ndef plot_decision_boundary(classifier: SVC, X: np.ndarray, y: np.ndarray) -&gt; go.Figure:\n    \"\"\"\n    Plot the decision boundary for a trained SVC classifier on 2D data.\n\n    Parameters:\n        classifier (SVC): Trained SVC classifier.\n        X (np.ndarray): 2D feature matrix of shape (n_samples, 2).\n        y (np.ndarray): Labels vector.\n\n    Returns:\n        go.Figure: Plotly figure with the decision boundary and data points.\n    \"\"\"\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    Z = classifier.decision_function(grid)\n    Z = Z.reshape(xx.shape)\n\n    fig = go.Figure()\n\n    # Contour for the decision function (decision boundary at level 0)\n    fig.add_trace(\n        go.Contour(\n            x=np.linspace(x_min, x_max, 300),\n            y=np.linspace(y_min, y_max, 300),\n            z=Z,\n            showscale=False,\n            colorscale=\"RdBu\",\n            opacity=0.7,\n            contours=dict(start=0, end=0, size=0.1, coloring=\"lines\"),\n            name=\"Decision Boundary\",\n        )\n    )\n\n    # Scatter plot for training data\n    fig.add_trace(\n        go.Scatter(\n            x=X[:, 0],\n            y=X[:, 1],\n            mode=\"markers\",\n            marker=dict(\n                color=y, colorscale=\"RdBu\", line=dict(width=1, color=\"black\"), size=8\n            ),\n            name=\"Data Points\",\n        )\n    )\n\n    # Highlight support vectors\n    sv = classifier.support_vectors_\n    fig.add_trace(\n        go.Scatter(\n            x=sv[:, 0],\n            y=sv[:, 1],\n            mode=\"markers\",\n            marker=dict(symbol=\"x\", color=\"black\", size=12, line=dict(width=2)),\n            name=\"Support Vectors\",\n        )\n    )\n\n    fig.update_layout(\n        title=\"SVM Decision Boundary\",\n        xaxis_title=\"Feature 1\",\n        yaxis_title=\"Feature 2\",\n        height=500,\n    )\n    return fig\n\n\nThe run_kernel_svm function loads the data, preprocesses it, trains the SVM with a selected kernel, and returns the visualization along with key SVM parameters.\n\n\nCode\ndef run_kernel_svm(\n    dataset_name: str, kernel_type: str, gamma: float, degree: int\n) -&gt; tuple[go.Figure, dict]:\n    \"\"\"\n    Run the SVM classification and return the decision boundary plot and classifier parameters.\n\n    Parameters:\n        dataset_name (str): The selected dataset ('Breast Cancer', 'Digits', 'Iris', 'Wine').\n        kernel_type (str): The type of kernel ('Gaussian', 'Polynomial', 'Linear').\n        gamma (float): Gamma parameter for the Gaussian kernel.\n        degree (int): Degree for the Polynomial kernel.\n\n    Returns:\n        Tuple[go.Figure, dict]: A Plotly figure and a JSON-like dict of classifier parameters.\n    \"\"\"\n    # Load and standardize the dataset\n    X, y = load_dataset(dataset_name)\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Reduce dimensionality to 2D for visualization if necessary\n    if X_scaled.shape[1] &gt; 2:\n        X_reduced = reduce_dimensionality(X_scaled, 2)\n    else:\n        X_reduced = X_scaled\n\n    # Map kernel type to scikit-learn's SVC kernel string\n    if kernel_type == \"Gaussian\":\n        kernel = \"rbf\"\n    elif kernel_type == \"Polynomial\":\n        kernel = \"poly\"\n    elif kernel_type == \"Linear\":\n        kernel = \"linear\"\n    else:\n        raise ValueError(\"Unsupported kernel type.\")\n\n    # Create and fit the SVC classifier\n    svc = SVC(\n        kernel=kernel,\n        gamma=gamma if kernel == \"rbf\" else \"scale\",\n        degree=degree if kernel == \"poly\" else 3,\n    )\n    svc.fit(X_reduced, y)\n\n    # Generate the decision boundary plot\n    fig = plot_decision_boundary(svc, X_reduced, y)\n\n    # Prepare SVM parameters for JSON output\n    result_info = {\n        \"dataset\": dataset_name,\n        \"kernel\": kernel_type,\n        \"n_samples\": int(len(y)),\n        \"n_support_vectors\": int(len(svc.support_)),\n        \"support_vectors\": svc.support_vectors_.tolist(),\n        \"dual_coef\": svc.dual_coef_.tolist(),\n        \"intercept\": svc.intercept_.tolist(),\n    }\n\n    return fig, result_info"
  },
  {
    "objectID": "content/06-clustering---svm.html#interactive-dashboard",
    "href": "content/06-clustering---svm.html#interactive-dashboard",
    "title": "Support Vector Machines",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\nThe dashboard allows you to adjust the dataset, kernel type, and kernel parameters below to interactively explore the decision boundaries and SVM parameters.\n\n\nCode\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"Interactive Support Vector Machine (SVM) Classifier\",\n) as demo:\n    dataset = gr.Dropdown(\n        choices=[\"Breast Cancer\", \"Digits\", \"Iris\", \"Wine\"], label=\"Dataset\"\n    )\n    kernel_type = gr.Radio(\n        choices=[\"Gaussian\", \"Polynomial\", \"Linear\"],\n        label=\"Kernel Type\",\n        value=\"Gaussian\",\n    )\n    gamma = gr.Slider(\n        minimum=0.01,\n        maximum=5,\n        step=0.01,\n        value=1.0,\n        label=\"Gamma (for Gaussian Kernel)\",\n    )\n    degree = gr.Slider(\n        minimum=1, maximum=10, step=1, value=3, label=\"Degree (for Polynomial Kernel)\"\n    )\n    run_button = gr.Button(\"Run SVM\")\n    plot_output = gr.Plot(label=\"Decision Boundary\")\n    json_output = gr.JSON(label=\"Classifier Parameters\")\n\n    run_button.click(\n        fn=run_kernel_svm,\n        inputs=[dataset, kernel_type, gamma, degree],\n        outputs=[plot_output, json_output],\n    )\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nimport gradio as gr\ndef load_dataset(name: str) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Load a dataset from scikit-learn.\n\n    Parameters:\n        name (str): Name of the dataset ('Breast Cancer', 'Digits', 'Iris', 'Wine').\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: Feature matrix X and label vector y.\n    \"\"\"\n    if name == \"Breast Cancer\":\n        data = datasets.load_breast_cancer()\n    elif name == \"Digits\":\n        data = datasets.load_digits()\n    elif name == \"Iris\":\n        data = datasets.load_iris()\n    elif name == \"Wine\":\n        data = datasets.load_wine()\n    else:\n        raise ValueError(\"Dataset not supported.\")\n\n    X = data.data\n    y = data.target\n    # For multi-class datasets, restrict to the first two classes for binary classification.\n    if len(np.unique(y)) &gt; 2:\n        mask = y &lt; 2\n        X = X[mask]\n        y = y[mask]\n    # Map labels: 0 -&gt; -1, 1 -&gt; 1\n    y = np.where(y == 0, -1, 1)\n    return X, y\n\n\ndef reduce_dimensionality(X: np.ndarray, n_components: int = 2) -&gt; np.ndarray:\n    \"\"\"\n    Reduce dimensionality of X using PCA.\n\n    Parameters:\n        X (np.ndarray): Feature matrix.\n        n_components (int): Number of principal components.\n\n    Returns:\n        np.ndarray: Data reduced to n_components dimensions.\n    \"\"\"\n    pca = PCA(n_components=n_components)\n    return pca.fit_transform(X)\ndef plot_decision_boundary(classifier: SVC, X: np.ndarray, y: np.ndarray) -&gt; go.Figure:\n    \"\"\"\n    Plot the decision boundary for a trained SVC classifier on 2D data.\n\n    Parameters:\n        classifier (SVC): Trained SVC classifier.\n        X (np.ndarray): 2D feature matrix of shape (n_samples, 2).\n        y (np.ndarray): Labels vector.\n\n    Returns:\n        go.Figure: Plotly figure with the decision boundary and data points.\n    \"\"\"\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    Z = classifier.decision_function(grid)\n    Z = Z.reshape(xx.shape)\n\n    fig = go.Figure()\n\n    # Contour for the decision function (decision boundary at level 0)\n    fig.add_trace(\n        go.Contour(\n            x=np.linspace(x_min, x_max, 300),\n            y=np.linspace(y_min, y_max, 300),\n            z=Z,\n            showscale=False,\n            colorscale=\"RdBu\",\n            opacity=0.7,\n            contours=dict(start=0, end=0, size=0.1, coloring=\"lines\"),\n            name=\"Decision Boundary\",\n        )\n    )\n\n    # Scatter plot for training data\n    fig.add_trace(\n        go.Scatter(\n            x=X[:, 0],\n            y=X[:, 1],\n            mode=\"markers\",\n            marker=dict(\n                color=y, colorscale=\"RdBu\", line=dict(width=1, color=\"black\"), size=8\n            ),\n            name=\"Data Points\",\n        )\n    )\n\n    # Highlight support vectors\n    sv = classifier.support_vectors_\n    fig.add_trace(\n        go.Scatter(\n            x=sv[:, 0],\n            y=sv[:, 1],\n            mode=\"markers\",\n            marker=dict(symbol=\"x\", color=\"black\", size=12, line=dict(width=2)),\n            name=\"Support Vectors\",\n        )\n    )\n\n    fig.update_layout(\n        title=\"SVM Decision Boundary\",\n        xaxis_title=\"Feature 1\",\n        yaxis_title=\"Feature 2\",\n        height=500,\n    )\n    return fig\ndef run_kernel_svm(\n    dataset_name: str, kernel_type: str, gamma: float, degree: int\n) -&gt; tuple[go.Figure, dict]:\n    \"\"\"\n    Run the SVM classification and return the decision boundary plot and classifier parameters.\n\n    Parameters:\n        dataset_name (str): The selected dataset ('Breast Cancer', 'Digits', 'Iris', 'Wine').\n        kernel_type (str): The type of kernel ('Gaussian', 'Polynomial', 'Linear').\n        gamma (float): Gamma parameter for the Gaussian kernel.\n        degree (int): Degree for the Polynomial kernel.\n\n    Returns:\n        Tuple[go.Figure, dict]: A Plotly figure and a JSON-like dict of classifier parameters.\n    \"\"\"\n    # Load and standardize the dataset\n    X, y = load_dataset(dataset_name)\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Reduce dimensionality to 2D for visualization if necessary\n    if X_scaled.shape[1] &gt; 2:\n        X_reduced = reduce_dimensionality(X_scaled, 2)\n    else:\n        X_reduced = X_scaled\n\n    # Map kernel type to scikit-learn's SVC kernel string\n    if kernel_type == \"Gaussian\":\n        kernel = \"rbf\"\n    elif kernel_type == \"Polynomial\":\n        kernel = \"poly\"\n    elif kernel_type == \"Linear\":\n        kernel = \"linear\"\n    else:\n        raise ValueError(\"Unsupported kernel type.\")\n\n    # Create and fit the SVC classifier\n    svc = SVC(\n        kernel=kernel,\n        gamma=gamma if kernel == \"rbf\" else \"scale\",\n        degree=degree if kernel == \"poly\" else 3,\n    )\n    svc.fit(X_reduced, y)\n\n    # Generate the decision boundary plot\n    fig = plot_decision_boundary(svc, X_reduced, y)\n\n    # Prepare SVM parameters for JSON output\n    result_info = {\n        \"dataset\": dataset_name,\n        \"kernel\": kernel_type,\n        \"n_samples\": int(len(y)),\n        \"n_support_vectors\": int(len(svc.support_)),\n        \"support_vectors\": svc.support_vectors_.tolist(),\n        \"dual_coef\": svc.dual_coef_.tolist(),\n        \"intercept\": svc.intercept_.tolist(),\n    }\n\n    return fig, result_info\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"Interactive Support Vector Machine (SVM) Classifier\",\n) as demo:\n    dataset = gr.Dropdown(\n        choices=[\"Breast Cancer\", \"Digits\", \"Iris\", \"Wine\"], label=\"Dataset\"\n    )\n    kernel_type = gr.Radio(\n        choices=[\"Gaussian\", \"Polynomial\", \"Linear\"],\n        label=\"Kernel Type\",\n        value=\"Gaussian\",\n    )\n    gamma = gr.Slider(\n        minimum=0.01,\n        maximum=5,\n        step=0.01,\n        value=1.0,\n        label=\"Gamma (for Gaussian Kernel)\",\n    )\n    degree = gr.Slider(\n        minimum=1, maximum=10, step=1, value=3, label=\"Degree (for Polynomial Kernel)\"\n    )\n    run_button = gr.Button(\"Run SVM\")\n    plot_output = gr.Plot(label=\"Decision Boundary\")\n    json_output = gr.JSON(label=\"Classifier Parameters\")\n\n    run_button.click(\n        fn=run_kernel_svm,\n        inputs=[dataset, kernel_type, gamma, degree],\n        outputs=[plot_output, json_output],\n    )\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/02-data-compression---dct.html",
    "href": "content/02-data-compression---dct.html",
    "title": "Discrete Cosine Transform",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gradio as gr\nfrom skimage import data\nfrom io import BytesIO\nimport PIL.Image\nimport plotly.graph_objects as go\nfrom typing import Optional"
  },
  {
    "objectID": "content/02-data-compression---dct.html#introduction",
    "href": "content/02-data-compression---dct.html#introduction",
    "title": "Discrete Cosine Transform",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis notebook illustrates the concepts behind JPEG compression by employing the 2D Discrete Cosine Transform (DCT) on a grayscale image. The algorithm operates by segmenting the image into smaller blocks, computing the DCT coefficients for each, and performing thresholding to zero out insignificant high-frequency coefficients. This process takes advantage of the fact that many coefficients, particularly those representing high frequencies, can be negligible and discarded without significantly affecting the perceived image quality, thus facilitating compression.\n\nImage Partitioning: For an image \\(A \\in \\mathbb{R}^{M \\times N}\\), we divide it into non-overlapping blocks of size \\(n \\times n\\) (typically \\(n = 8\\)).\n2D DCT for a Block: The transform for a block \\(B\\) is defined as:\n\n\\[\nc_{k,l} = \\alpha(k)\\alpha(l) \\sum_{r=0}^{n-1} \\sum_{s=0}^{n-1} B_{r,s} \\cos\\left(\\frac{\\pi (2r+1)k}{2n}\\right) \\cos\\left(\\frac{\\pi (2s+1)l}{2n}\\right)\n\\]\nwhere the scaling factor \\(\\alpha(k)\\) is given by:\n\\[\n\\alpha(k) =\n\\begin{cases}\n\\sqrt{\\frac{1}{n}}, & k = 0, \\\\\n\\sqrt{\\frac{2}{n}}, & k &gt; 0.\n\\end{cases}\n\\]\n\nInverse DCT (IDCT): Reconstructed via:\n\n\\[\nB_{r,s} = \\sum_{k=0}^{n-1} \\sum_{l=0}^{n-1} \\alpha(k)\\alpha(l) \\, c_{k,l} \\cos\\left(\\frac{\\pi (2r+1)k}{2n}\\right) \\cos\\left(\\frac{\\pi (2s+1)l}{2n}\\right).\n\\]\n\nThresholding: Through thresholding, small coefficients are nullified, maintaining core low-frequency content while reducing data size.\n\nThe interactive dashboard enables you to adjust:\n\nThreshold: Minimum coefficient value retained during compression.\nBlock Size: Controls the granularity of the transformation.\nImage Input: Upload an image or select from sample options like cameraman, coins, or moon.\n\nYouâ€™ll observe:\n\nOriginal vs Reconstructed Images: Visual comparison post-compression.\nDCT Coefficients Heatmap: First block coefficient visualization using Plotly.\nCompression Statistics: JSON data illustrating nonzero coefficients and compression ratios."
  },
  {
    "objectID": "content/02-data-compression---dct.html#python-implementation",
    "href": "content/02-data-compression---dct.html#python-implementation",
    "title": "Discrete Cosine Transform",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\nLetâ€™s begin by constructing the DCT transformation matrix \\(T\\) with dimensions \\(n \\times n\\), whose elements are derived as:\n\\[\nT_{k,r} = \\alpha(k) \\cos\\left(\\frac{\\pi (2r+1)k}{2n}\\right)\n\\]\nThe orthonormal property of \\(T\\) ensures that the DCT can be expressed as:\n\\[\nC = T \\, B \\, T^\\top\n\\]\nConsequently, we retrieve the original block through:\n\\[\nB = T^\\top \\, C \\, T\n\\]\nHereâ€™s the implementation for generating the transformation matrix and applying DCT and IDCT:\n\n\nCode\ndef dct_matrix(n: int) -&gt; np.ndarray:\n    \"\"\"\n    Generate the DCT transformation matrix of size n x n.\n\n    Args:\n        n: Block size.\n\n    Returns:\n        A numpy array representing the DCT matrix.\n    \"\"\"\n    T = np.empty((n, n))\n    factor = np.pi / (2 * n)\n    for k in range(n):\n        for r in range(n):\n            alpha = np.sqrt(1 / n) if k == 0 else np.sqrt(2 / n)\n            T[k, r] = alpha * np.cos((2 * r + 1) * k * factor)\n    return T\n\n\ndef dct2(block: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the 2D DCT of an n x n block.\n\n    Args:\n        block: A 2D numpy array.\n\n    Returns:\n        The DCT coefficients as a 2D numpy array.\n    \"\"\"\n    n = block.shape[0]\n    T = dct_matrix(n)\n    return T @ block @ T.T\n\n\ndef idct2(coeff: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the 2D inverse DCT of an n x n coefficient block.\n\n    Args:\n        coeff: A 2D numpy array of DCT coefficients.\n\n    Returns:\n        The reconstructed block as a 2D numpy array.\n    \"\"\"\n    n = coeff.shape[0]\n    T = dct_matrix(n)\n    return T.T @ coeff @ T\n\n\nTo decompress an image, it is segmented into blocks, which are processed individually before reassembling the full image.\n\n\nCode\ndef image_to_blocks(img: np.ndarray, block_size: int) -&gt; list:\n    \"\"\"\n    Decompose the image into non-overlapping blocks of size block_size x block_size.\n\n    Args:\n        img: A 2D numpy array representing the grayscale image.\n        block_size: The size of each block.\n\n    Returns:\n        A list of lists of blocks.\n    \"\"\"\n    M, N = img.shape\n    blocks = []\n    for i in range(0, M, block_size):\n        row = []\n        for j in range(0, N, block_size):\n            block = img[i : i + block_size, j : j + block_size]\n            # Pad if necessary so that every block is block_size x block_size.\n            if block.shape != (block_size, block_size):\n                block = np.pad(\n                    block,\n                    (\n                        (0, block_size - block.shape[0]),\n                        (0, block_size - block.shape[1]),\n                    ),\n                    mode=\"constant\",\n                )\n            row.append(block.astype(np.float32))\n        blocks.append(row)\n    return blocks\n\n\ndef blocks_to_image(blocks: list) -&gt; np.ndarray:\n    \"\"\"\n    Reconstruct the image from blocks.\n\n    Args:\n        blocks: A list of lists of 2D numpy arrays.\n\n    Returns:\n        The reconstructed image as a 2D numpy array.\n    \"\"\"\n    row_images = [np.hstack(row) for row in blocks]\n    return np.vstack(row_images)\n\n\ndef apply_dct_to_blocks(blocks: list) -&gt; list:\n    \"\"\"\n    Apply 2D DCT to each image block.\n\n    Args:\n        blocks: A list of lists of image blocks.\n\n    Returns:\n        A list of lists with the DCT-transformed blocks.\n    \"\"\"\n    return [[dct2(block) for block in row] for row in blocks]\n\n\ndef apply_idct_to_blocks(blocks: list) -&gt; list:\n    \"\"\"\n    Apply 2D inverse DCT to each block.\n\n    Args:\n        blocks: A list of lists of DCT coefficient blocks.\n\n    Returns:\n        A list of lists with the inverse-transformed blocks.\n    \"\"\"\n    return [[idct2(block) for block in row] for row in blocks]\n\n\nThresholding is essential for compression by zeroing out small coefficients while preserving important information.\n\n\nCode\ndef threshold_coefficients(dct_blocks: list, threshold: float) -&gt; tuple:\n    \"\"\"\n    Apply thresholding to DCT coefficients in each block.\n\n    Args:\n        dct_blocks: A list of lists of DCT coefficient blocks.\n        threshold: The threshold value. Coefficients with absolute value below this are set to zero.\n\n    Returns:\n        A tuple containing:\n         - The thresholded blocks.\n         - The total count of nonzero coefficients.\n    \"\"\"\n    nonzero_count = 0\n    new_blocks = []\n    for row in dct_blocks:\n        new_row = []\n        for block in row:\n            mask = np.abs(block) &gt; threshold\n            nonzero_count += np.count_nonzero(mask)\n            new_row.append(block * mask)\n        new_blocks.append(new_row)\n    return new_blocks, nonzero_count\n\n\n\n\nCode\ndef load_input_image(\n    uploaded_image: Optional[object], sample_choice: str\n) -&gt; np.ndarray:\n    \"\"\"\n    Load the input image from an upload or sample selection and convert it to grayscale.\n\n    Args:\n        uploaded_image: The uploaded image (as a numpy array or PIL.Image) or None.\n        sample_choice: The name of the sample image to use if no upload is provided.\n\n    Returns:\n        A 2D numpy array representing the grayscale image.\n    \"\"\"\n    if uploaded_image is not None:\n        # Check if the uploaded image is a numpy array\n        if isinstance(uploaded_image, np.ndarray):\n            img = uploaded_image\n        else:\n            # Assume it's a PIL image\n            img = np.array(uploaded_image)\n        # Convert to grayscale if needed (if image has 3 channels)\n        if img.ndim == 3:\n            if img.shape[2] &gt;= 3:\n                # Use standard luminance conversion\n                img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n            else:\n                img = img[..., 0]\n        return img.astype(np.uint8)\n    else:\n        # Load sample image from skimage.data\n        if sample_choice.lower() == \"cameraman\":\n            return data.camera()\n        elif sample_choice.lower() == \"coins\":\n            return data.coins()\n        elif sample_choice.lower() == \"moon\":\n            return data.moon()\n        else:\n            # Default to cameraman if unknown choice.\n            return data.camera()\n\n\n\n\nCode\ndef plotly_heatmap(coeff_block: np.ndarray) -&gt; go.Figure:\n    \"\"\"\n    Generate a Plotly heatmap for a given DCT coefficient block.\n\n    Args:\n        coeff_block: A 2D numpy array of DCT coefficients.\n\n    Returns:\n        A Plotly Figure object displaying the heatmap.\n    \"\"\"\n    fig = go.Figure(data=go.Heatmap(z=coeff_block, colorscale=\"Viridis\"))\n    fig.update_layout(\n        title=\"DCT Coefficients (First Block)\",\n        xaxis_title=\"Coefficient index\",\n        yaxis_title=\"Coefficient index\",\n        margin=dict(l=20, r=20, t=40, b=20),\n    )\n    return fig\n\n\nprocess_image orchestrates the entire workflow: from loading the image, transforming it with DCT, thresholding, reconstructing using IDCT, and generating the heatmap and statistics.\n\n\nCode\ndef process_image(\n    threshold: float,\n    block_size: int = 8,\n    uploaded_image: Optional[object] = None,\n    sample_choice: str = \"cameraman\",\n) -&gt; tuple:\n    \"\"\"\n    Process the input image using block-wise 2D DCT, thresholding, and IDCT reconstruction.\n\n    Args:\n        threshold: The threshold value for the DCT coefficients.\n        block_size: The size of each block (default 8).\n        uploaded_image: The uploaded image (if any).\n        sample_choice: The sample image to use if no upload is provided.\n\n    Returns:\n        A tuple containing:\n         - The original image (as a 2D numpy array).\n         - The reconstructed image after thresholding.\n         - A Plotly Figure object for the DCT coefficients heatmap (first block).\n         - A JSON-serializable dictionary with compression statistics.\n    \"\"\"\n    # Load input image (upload takes precedence over sample choice)\n    img = load_input_image(uploaded_image, sample_choice)\n    M, N = img.shape\n    # Crop the image so that its dimensions are multiples of block_size\n    M_new = (M // block_size) * block_size\n    N_new = (N // block_size) * block_size\n    img = img[:M_new, :N_new]\n\n    # Partition the image into blocks\n    blocks = image_to_blocks(img, block_size)\n\n    # Apply the 2D DCT to each block\n    dct_blocks = apply_dct_to_blocks(blocks)\n\n    total_coeffs = M_new * N_new  # one coefficient per pixel\n\n    # Threshold the DCT coefficients\n    thresh_blocks, nonzero_count = threshold_coefficients(dct_blocks, threshold)\n\n    # Reconstruct the image using inverse DCT\n    idct_blocks = apply_idct_to_blocks(thresh_blocks)\n    reconstructed = blocks_to_image(idct_blocks)\n\n    # Prepare a summary of the compression statistics\n    summary = {\n        \"block_size\": block_size,\n        \"image_shape\": {\"rows\": int(M_new), \"cols\": int(N_new)},\n        \"total_coefficients\": int(total_coeffs),\n        \"nonzero_coefficients\": int(nonzero_count),\n        \"compression_ratio\": float(nonzero_count / total_coeffs),\n    }\n\n    # Generate a responsive Plotly heatmap for the DCT coefficients of the first block\n    first_block = dct_blocks[0][0]\n    heatmap_fig = plotly_heatmap(first_block)\n\n    return img, reconstructed, heatmap_fig, summary"
  },
  {
    "objectID": "content/02-data-compression---dct.html#interactive-dashboard",
    "href": "content/02-data-compression---dct.html#interactive-dashboard",
    "title": "Discrete Cosine Transform",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\nNow, letâ€™s engage with the interactive dashboard, empowering users to fine-tune the DCT compression threshold and block size, and effortlessly switch between image uploads and preset samples. It presents:\n\nSide-by-Side Image Comparison: Examine the contrast between original and reconstructed images.\nDCT Coefficients Heatmap: Visualize the spectra of the first block.\nCompression Insights: Powered by JSON data, revealing metrics like nonzero coefficient tally and compression efficiency.\n\n\n\nCode\ndef gradio_interface(\n    threshold: float,\n    block_size: int,\n    uploaded_image: Optional[object],\n    sample_choice: str,\n) -&gt; tuple:\n    \"\"\"\n    Gradio interface function to process the image and return outputs.\n\n    Args:\n        threshold: The threshold value for DCT coefficients.\n        block_size: The block size for image decomposition.\n        uploaded_image: The uploaded image (if provided).\n        sample_choice: The sample image to use if no upload is provided.\n\n    Returns:\n        A tuple containing:\n         - A combined image (side-by-side original and reconstructed).\n         - A Plotly Figure for the DCT coefficients heatmap.\n         - A dictionary with compression statistics.\n    \"\"\"\n    original, reconstructed, heatmap_fig, summary = process_image(\n        threshold, block_size, uploaded_image, sample_choice\n    )\n\n    # Create a side-by-side visualization of the original and reconstructed images.\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(original, cmap=\"gray\")\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis(\"off\")\n    axes[1].imshow(reconstructed, cmap=\"gray\")\n    axes[1].set_title(\"Reconstructed Image\")\n    axes[1].axis(\"off\")\n    plt.tight_layout()\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\")\n    buf.seek(0)\n    combined_img = PIL.Image.open(buf)\n    plt.close()\n\n    return combined_img, heatmap_fig, summary\n\n\nwith gr.Blocks(css=\"\"\"gradio-app {background: #222222 !important}\"\"\") as demo:\n    gr.Markdown(\n        \"\"\"\n    # JPEG Compression via Discrete Cosine Transform (DCT)\n    \"\"\"\n    )\n\n    with gr.Row():\n        threshold_slider = gr.Slider(\n            minimum=0, maximum=100, step=0.5, value=10, label=\"Threshold\"\n        )\n        block_size_input = gr.Number(value=8, label=\"Block Size (n x n)\", precision=0)\n\n    uploaded_image_input = gr.Image(type=\"numpy\", label=\"Upload your image (optional)\")\n    sample_choice_input = gr.Dropdown(\n        choices=[\"cameraman\", \"coins\", \"moon\"],\n        value=\"cameraman\",\n        label=\"Or select a sample image\",\n    )\n\n    output_combined = gr.Image(label=\"Original vs Reconstructed Image\")\n    output_heatmap = gr.Plot(label=\"DCT Coefficients Heatmap (First Block)\")\n    output_stats = gr.JSON(label=\"Compression Statistics\")\n\n    btn = gr.Button(\"Apply Compression\")\n\n    btn.click(\n        fn=gradio_interface,\n        inputs=[\n            threshold_slider,\n            block_size_input,\n            uploaded_image_input,\n            sample_choice_input,\n        ],\n        outputs=[output_combined, output_heatmap, output_stats],\n    )\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gradio as gr\nfrom skimage import data\nfrom io import BytesIO\nimport PIL.Image\nimport plotly.graph_objects as go\nfrom typing import Optional\ndef dct_matrix(n: int) -&gt; np.ndarray:\n    \"\"\"\n    Generate the DCT transformation matrix of size n x n.\n\n    Args:\n        n: Block size.\n\n    Returns:\n        A numpy array representing the DCT matrix.\n    \"\"\"\n    T = np.empty((n, n))\n    factor = np.pi / (2 * n)\n    for k in range(n):\n        for r in range(n):\n            alpha = np.sqrt(1 / n) if k == 0 else np.sqrt(2 / n)\n            T[k, r] = alpha * np.cos((2 * r + 1) * k * factor)\n    return T\n\n\ndef dct2(block: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the 2D DCT of an n x n block.\n\n    Args:\n        block: A 2D numpy array.\n\n    Returns:\n        The DCT coefficients as a 2D numpy array.\n    \"\"\"\n    n = block.shape[0]\n    T = dct_matrix(n)\n    return T @ block @ T.T\n\n\ndef idct2(coeff: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the 2D inverse DCT of an n x n coefficient block.\n\n    Args:\n        coeff: A 2D numpy array of DCT coefficients.\n\n    Returns:\n        The reconstructed block as a 2D numpy array.\n    \"\"\"\n    n = coeff.shape[0]\n    T = dct_matrix(n)\n    return T.T @ coeff @ T\ndef image_to_blocks(img: np.ndarray, block_size: int) -&gt; list:\n    \"\"\"\n    Decompose the image into non-overlapping blocks of size block_size x block_size.\n\n    Args:\n        img: A 2D numpy array representing the grayscale image.\n        block_size: The size of each block.\n\n    Returns:\n        A list of lists of blocks.\n    \"\"\"\n    M, N = img.shape\n    blocks = []\n    for i in range(0, M, block_size):\n        row = []\n        for j in range(0, N, block_size):\n            block = img[i : i + block_size, j : j + block_size]\n            # Pad if necessary so that every block is block_size x block_size.\n            if block.shape != (block_size, block_size):\n                block = np.pad(\n                    block,\n                    (\n                        (0, block_size - block.shape[0]),\n                        (0, block_size - block.shape[1]),\n                    ),\n                    mode=\"constant\",\n                )\n            row.append(block.astype(np.float32))\n        blocks.append(row)\n    return blocks\n\n\ndef blocks_to_image(blocks: list) -&gt; np.ndarray:\n    \"\"\"\n    Reconstruct the image from blocks.\n\n    Args:\n        blocks: A list of lists of 2D numpy arrays.\n\n    Returns:\n        The reconstructed image as a 2D numpy array.\n    \"\"\"\n    row_images = [np.hstack(row) for row in blocks]\n    return np.vstack(row_images)\n\n\ndef apply_dct_to_blocks(blocks: list) -&gt; list:\n    \"\"\"\n    Apply 2D DCT to each image block.\n\n    Args:\n        blocks: A list of lists of image blocks.\n\n    Returns:\n        A list of lists with the DCT-transformed blocks.\n    \"\"\"\n    return [[dct2(block) for block in row] for row in blocks]\n\n\ndef apply_idct_to_blocks(blocks: list) -&gt; list:\n    \"\"\"\n    Apply 2D inverse DCT to each block.\n\n    Args:\n        blocks: A list of lists of DCT coefficient blocks.\n\n    Returns:\n        A list of lists with the inverse-transformed blocks.\n    \"\"\"\n    return [[idct2(block) for block in row] for row in blocks]\ndef threshold_coefficients(dct_blocks: list, threshold: float) -&gt; tuple:\n    \"\"\"\n    Apply thresholding to DCT coefficients in each block.\n\n    Args:\n        dct_blocks: A list of lists of DCT coefficient blocks.\n        threshold: The threshold value. Coefficients with absolute value below this are set to zero.\n\n    Returns:\n        A tuple containing:\n         - The thresholded blocks.\n         - The total count of nonzero coefficients.\n    \"\"\"\n    nonzero_count = 0\n    new_blocks = []\n    for row in dct_blocks:\n        new_row = []\n        for block in row:\n            mask = np.abs(block) &gt; threshold\n            nonzero_count += np.count_nonzero(mask)\n            new_row.append(block * mask)\n        new_blocks.append(new_row)\n    return new_blocks, nonzero_count\ndef load_input_image(\n    uploaded_image: Optional[object], sample_choice: str\n) -&gt; np.ndarray:\n    \"\"\"\n    Load the input image from an upload or sample selection and convert it to grayscale.\n\n    Args:\n        uploaded_image: The uploaded image (as a numpy array or PIL.Image) or None.\n        sample_choice: The name of the sample image to use if no upload is provided.\n\n    Returns:\n        A 2D numpy array representing the grayscale image.\n    \"\"\"\n    if uploaded_image is not None:\n        # Check if the uploaded image is a numpy array\n        if isinstance(uploaded_image, np.ndarray):\n            img = uploaded_image\n        else:\n            # Assume it's a PIL image\n            img = np.array(uploaded_image)\n        # Convert to grayscale if needed (if image has 3 channels)\n        if img.ndim == 3:\n            if img.shape[2] &gt;= 3:\n                # Use standard luminance conversion\n                img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n            else:\n                img = img[..., 0]\n        return img.astype(np.uint8)\n    else:\n        # Load sample image from skimage.data\n        if sample_choice.lower() == \"cameraman\":\n            return data.camera()\n        elif sample_choice.lower() == \"coins\":\n            return data.coins()\n        elif sample_choice.lower() == \"moon\":\n            return data.moon()\n        else:\n            # Default to cameraman if unknown choice.\n            return data.camera()\ndef plotly_heatmap(coeff_block: np.ndarray) -&gt; go.Figure:\n    \"\"\"\n    Generate a Plotly heatmap for a given DCT coefficient block.\n\n    Args:\n        coeff_block: A 2D numpy array of DCT coefficients.\n\n    Returns:\n        A Plotly Figure object displaying the heatmap.\n    \"\"\"\n    fig = go.Figure(data=go.Heatmap(z=coeff_block, colorscale=\"Viridis\"))\n    fig.update_layout(\n        title=\"DCT Coefficients (First Block)\",\n        xaxis_title=\"Coefficient index\",\n        yaxis_title=\"Coefficient index\",\n        margin=dict(l=20, r=20, t=40, b=20),\n    )\n    return fig\ndef process_image(\n    threshold: float,\n    block_size: int = 8,\n    uploaded_image: Optional[object] = None,\n    sample_choice: str = \"cameraman\",\n) -&gt; tuple:\n    \"\"\"\n    Process the input image using block-wise 2D DCT, thresholding, and IDCT reconstruction.\n\n    Args:\n        threshold: The threshold value for the DCT coefficients.\n        block_size: The size of each block (default 8).\n        uploaded_image: The uploaded image (if any).\n        sample_choice: The sample image to use if no upload is provided.\n\n    Returns:\n        A tuple containing:\n         - The original image (as a 2D numpy array).\n         - The reconstructed image after thresholding.\n         - A Plotly Figure object for the DCT coefficients heatmap (first block).\n         - A JSON-serializable dictionary with compression statistics.\n    \"\"\"\n    # Load input image (upload takes precedence over sample choice)\n    img = load_input_image(uploaded_image, sample_choice)\n    M, N = img.shape\n    # Crop the image so that its dimensions are multiples of block_size\n    M_new = (M // block_size) * block_size\n    N_new = (N // block_size) * block_size\n    img = img[:M_new, :N_new]\n\n    # Partition the image into blocks\n    blocks = image_to_blocks(img, block_size)\n\n    # Apply the 2D DCT to each block\n    dct_blocks = apply_dct_to_blocks(blocks)\n\n    total_coeffs = M_new * N_new  # one coefficient per pixel\n\n    # Threshold the DCT coefficients\n    thresh_blocks, nonzero_count = threshold_coefficients(dct_blocks, threshold)\n\n    # Reconstruct the image using inverse DCT\n    idct_blocks = apply_idct_to_blocks(thresh_blocks)\n    reconstructed = blocks_to_image(idct_blocks)\n\n    # Prepare a summary of the compression statistics\n    summary = {\n        \"block_size\": block_size,\n        \"image_shape\": {\"rows\": int(M_new), \"cols\": int(N_new)},\n        \"total_coefficients\": int(total_coeffs),\n        \"nonzero_coefficients\": int(nonzero_count),\n        \"compression_ratio\": float(nonzero_count / total_coeffs),\n    }\n\n    # Generate a responsive Plotly heatmap for the DCT coefficients of the first block\n    first_block = dct_blocks[0][0]\n    heatmap_fig = plotly_heatmap(first_block)\n\n    return img, reconstructed, heatmap_fig, summary\ndef gradio_interface(\n    threshold: float,\n    block_size: int,\n    uploaded_image: Optional[object],\n    sample_choice: str,\n) -&gt; tuple:\n    \"\"\"\n    Gradio interface function to process the image and return outputs.\n\n    Args:\n        threshold: The threshold value for DCT coefficients.\n        block_size: The block size for image decomposition.\n        uploaded_image: The uploaded image (if provided).\n        sample_choice: The sample image to use if no upload is provided.\n\n    Returns:\n        A tuple containing:\n         - A combined image (side-by-side original and reconstructed).\n         - A Plotly Figure for the DCT coefficients heatmap.\n         - A dictionary with compression statistics.\n    \"\"\"\n    original, reconstructed, heatmap_fig, summary = process_image(\n        threshold, block_size, uploaded_image, sample_choice\n    )\n\n    # Create a side-by-side visualization of the original and reconstructed images.\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(original, cmap=\"gray\")\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis(\"off\")\n    axes[1].imshow(reconstructed, cmap=\"gray\")\n    axes[1].set_title(\"Reconstructed Image\")\n    axes[1].axis(\"off\")\n    plt.tight_layout()\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\")\n    buf.seek(0)\n    combined_img = PIL.Image.open(buf)\n    plt.close()\n\n    return combined_img, heatmap_fig, summary\n\n\nwith gr.Blocks(css=\"\"\"gradio-app {background: #222222 !important}\"\"\") as demo:\n    gr.Markdown(\n        \"\"\"\n    # JPEG Compression via Discrete Cosine Transform (DCT)\n    \"\"\"\n    )\n\n    with gr.Row():\n        threshold_slider = gr.Slider(\n            minimum=0, maximum=100, step=0.5, value=10, label=\"Threshold\"\n        )\n        block_size_input = gr.Number(value=8, label=\"Block Size (n x n)\", precision=0)\n\n    uploaded_image_input = gr.Image(type=\"numpy\", label=\"Upload your image (optional)\")\n    sample_choice_input = gr.Dropdown(\n        choices=[\"cameraman\", \"coins\", \"moon\"],\n        value=\"cameraman\",\n        label=\"Or select a sample image\",\n    )\n\n    output_combined = gr.Image(label=\"Original vs Reconstructed Image\")\n    output_heatmap = gr.Plot(label=\"DCT Coefficients Heatmap (First Block)\")\n    output_stats = gr.JSON(label=\"Compression Statistics\")\n\n    btn = gr.Button(\"Apply Compression\")\n\n    btn.click(\n        fn=gradio_interface,\n        inputs=[\n            threshold_slider,\n            block_size_input,\n            uploaded_image_input,\n            sample_choice_input,\n        ],\n        outputs=[output_combined, output_heatmap, output_stats],\n    )\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/03-image-processing---denoising.html",
    "href": "content/03-image-processing---denoising.html",
    "title": "Denoising",
    "section": "",
    "text": "Code\nimport numpy as np\nimport plotly.express as px\nimport gradio as gr\nfrom skimage import color, data, img_as_float\nfrom typing import Any"
  },
  {
    "objectID": "content/03-image-processing---denoising.html#introduction",
    "href": "content/03-image-processing---denoising.html#introduction",
    "title": "Denoising",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this notebook, we explore the concept of Denoising with a focus on mitigating Salt & Pepper noise using a Median Filter. This type of noise and filtering operation is a staple in image processing for enhancing image quality.\n\nSalt & Pepper Noise:\n\nThis noise manifests as random pixel corruption, where a fraction \\(\\sigma\\) of pixels in a grayscale image \\(f\\) are unexpectedly set to either 0 (â€œpepperâ€) or 1 (â€œsaltâ€). Specifically, out of \\(N\\) total pixels, \\(N \\cdot \\sigma\\) will be affected. This noise drastically affects image quality without altering structural content.\n\nMedian Filtering:\n\nWe employ a median filter over a sliding window of size \\(s\\). For each pixel location \\((i,j)\\), the filter replaces its value with the median from the surrounding pixel values:\n\n\\[\ng(i,j) = \\operatorname{median}\\{f(k,l) \\mid (k,l) \\in W_{ij}\\}.\n\\]\n\nThis method is effective in noise reduction while preserving significant edges and details, due to the non-linear nature of the median statistic."
  },
  {
    "objectID": "content/03-image-processing---denoising.html#python-implementation",
    "href": "content/03-image-processing---denoising.html#python-implementation",
    "title": "Denoising",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\nThe following implementation illustrates how we inject salt & pepper noise into an image. This function generates noise by random pixel corruption:\n\n\nCode\ndef add_salt_and_pepper_noise(image: np.ndarray, noise_ratio: float) -&gt; np.ndarray:\n    \"\"\"\n    Adds salt & pepper noise to a grayscale image.\n\n    Parameters:\n        image (np.ndarray): 2D numpy array with values in [0,1].\n        noise_ratio (float): Fraction of pixels to be corrupted.\n\n    Returns:\n        np.ndarray: Noisy image.\n    \"\"\"\n    noisy = image.copy()\n    total_pixels = image.size\n    num_noisy = int(total_pixels * noise_ratio)\n\n    # Generate random indices for the pixels to corrupt\n    coords = np.unravel_index(\n        np.random.choice(total_pixels, num_noisy, replace=False), image.shape\n    )\n    # Randomly assign salt (1.0) or pepper (0.0)\n    noisy[coords] = np.random.choice([0.0, 1.0], size=num_noisy)\n    return noisy\n\n\nThe median_filter function implements the denoising technique by applying a median operation over a locally sliding window. This function is crucial for preserving edges:\n\n\nCode\ndef median_filter(image: np.ndarray, window_size: int) -&gt; np.ndarray:\n    \"\"\"\n    Applies a median filter to a grayscale image.\n\n    Parameters:\n        image (np.ndarray): 2D numpy array with values in [0,1].\n        window_size (int): Size of the square window (must be odd).\n\n    Returns:\n        np.ndarray: Denoised image.\n    \"\"\"\n    if window_size % 2 == 0:\n        raise ValueError(\"Window size must be odd.\")\n\n    pad_width = window_size // 2\n    padded = np.pad(image, pad_width, mode=\"edge\")\n    output = np.zeros_like(image)\n\n    for i in range(output.shape[0]):\n        for j in range(output.shape[1]):\n            window = padded[i : i + window_size, j : j + window_size]\n            output[i, j] = np.median(window)\n    return output"
  },
  {
    "objectID": "content/03-image-processing---denoising.html#interactive-dashboard",
    "href": "content/03-image-processing---denoising.html#interactive-dashboard",
    "title": "Denoising",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\nThe Gradio-powered dashboard provides an interactive avenue for experimenting with noise levels and filter window sizes on uploaded or default images. Components include:\n\nImage Upload and Processing: Users can add custom images or use a default one.\nAdjustable Noise Ratio & Filter Size: Controls to vary noise and window parameters.\nVisual Comparison: Side-by-side comparison of the noisy and denoised outputs.\nProcessing Insights: JSON output details the parameters and steps applied.\n\n\n\nCode\nDEFAULT_IMAGE = img_as_float(data.camera())\n\n\n\n\nCode\ndef process_image(\n    image: np.ndarray, noise_ratio: float, window_size: int\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the uploaded image by adding salt & pepper noise and then denoising it.\n\n    Parameters:\n        image (np.ndarray): Uploaded image in numpy array format.\n        noise_ratio (float): Fraction of pixels to corrupt with noise.\n        window_size (int): Median filter window size (must be odd).\n\n    Returns:\n        dict: Dictionary containing Plotly figures for the noisy and denoised images,\n              along with JSON details of the process.\n    \"\"\"\n    # Convert image to float and grayscale if necessary\n    if image.ndim == 3:\n        image_gray = color.rgb2gray(image)\n    else:\n        image_gray = image.copy()\n\n    image_gray = img_as_float(image_gray)\n\n    # Ensure window size is odd\n    if window_size % 2 == 0:\n        window_size += 1\n\n    # Apply noise and denoising\n    noisy_img = add_salt_and_pepper_noise(image_gray, noise_ratio)\n    denoised_img = median_filter(noisy_img, window_size)\n\n    # Create Plotly figures\n    fig_noisy = px.imshow(noisy_img, color_continuous_scale=\"gray\", title=\"Noisy Image\")\n    fig_denoised = px.imshow(\n        denoised_img, color_continuous_scale=\"gray\", title=\"Denoised Image\"\n    )\n\n    details = {\n        \"Noise Ratio\": noise_ratio,\n        \"Median Filter Window Size\": window_size,\n        \"Original Image Shape\": image_gray.shape,\n    }\n\n    return {\n        \"Noisy Image\": fig_noisy,\n        \"Denoised Image\": fig_denoised,\n        \"Details\": details,\n    }\n\n\nwith gr.Blocks(\n    title=\"Image Denoising via Median Filtering\",\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n) as demo:\n    image_input = gr.Image(label=\"Upload Image\", value=DEFAULT_IMAGE)\n    with gr.Row():\n        noise_ratio_slider = gr.Slider(\n            minimum=0.0, maximum=0.5, step=0.01, value=0.1, label=\"Noise Ratio\"\n        )\n        window_size_slider = gr.Slider(\n            minimum=3,\n            maximum=21,\n            step=2,\n            value=3,\n            label=\"Median Filter Window Size (odd)\",\n        )\n\n    process_button = gr.Button(\"Apply Denoising\")\n\n    with gr.Row():\n        output_noisy = gr.Plot(label=\"Noisy Image\")\n        output_denoised = gr.Plot(label=\"Denoised Image\")\n\n    output_details = gr.JSON(label=\"Processing Details\")\n\n    def process_wrapper(image, noise_ratio, window_size):\n        # If no image is uploaded, use a default image from skimage\n        if image is None:\n            from skimage import data\n\n            image = img_as_float(data.camera())\n        results = process_image(image, noise_ratio, int(window_size))\n        return results[\"Noisy Image\"], results[\"Denoised Image\"], results[\"Details\"]\n\n    process_button.click(\n        process_wrapper,\n        inputs=[image_input, noise_ratio_slider, window_size_slider],\n        outputs=[output_noisy, output_denoised, output_details],\n    )\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport numpy as np\nimport plotly.express as px\nimport gradio as gr\nfrom skimage import color, data, img_as_float\nfrom typing import Any\ndef add_salt_and_pepper_noise(image: np.ndarray, noise_ratio: float) -&gt; np.ndarray:\n    \"\"\"\n    Adds salt & pepper noise to a grayscale image.\n\n    Parameters:\n        image (np.ndarray): 2D numpy array with values in [0,1].\n        noise_ratio (float): Fraction of pixels to be corrupted.\n\n    Returns:\n        np.ndarray: Noisy image.\n    \"\"\"\n    noisy = image.copy()\n    total_pixels = image.size\n    num_noisy = int(total_pixels * noise_ratio)\n\n    # Generate random indices for the pixels to corrupt\n    coords = np.unravel_index(\n        np.random.choice(total_pixels, num_noisy, replace=False), image.shape\n    )\n    # Randomly assign salt (1.0) or pepper (0.0)\n    noisy[coords] = np.random.choice([0.0, 1.0], size=num_noisy)\n    return noisy\ndef median_filter(image: np.ndarray, window_size: int) -&gt; np.ndarray:\n    \"\"\"\n    Applies a median filter to a grayscale image.\n\n    Parameters:\n        image (np.ndarray): 2D numpy array with values in [0,1].\n        window_size (int): Size of the square window (must be odd).\n\n    Returns:\n        np.ndarray: Denoised image.\n    \"\"\"\n    if window_size % 2 == 0:\n        raise ValueError(\"Window size must be odd.\")\n\n    pad_width = window_size // 2\n    padded = np.pad(image, pad_width, mode=\"edge\")\n    output = np.zeros_like(image)\n\n    for i in range(output.shape[0]):\n        for j in range(output.shape[1]):\n            window = padded[i : i + window_size, j : j + window_size]\n            output[i, j] = np.median(window)\n    return output\nDEFAULT_IMAGE = img_as_float(data.camera())\ndef process_image(\n    image: np.ndarray, noise_ratio: float, window_size: int\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the uploaded image by adding salt & pepper noise and then denoising it.\n\n    Parameters:\n        image (np.ndarray): Uploaded image in numpy array format.\n        noise_ratio (float): Fraction of pixels to corrupt with noise.\n        window_size (int): Median filter window size (must be odd).\n\n    Returns:\n        dict: Dictionary containing Plotly figures for the noisy and denoised images,\n              along with JSON details of the process.\n    \"\"\"\n    # Convert image to float and grayscale if necessary\n    if image.ndim == 3:\n        image_gray = color.rgb2gray(image)\n    else:\n        image_gray = image.copy()\n\n    image_gray = img_as_float(image_gray)\n\n    # Ensure window size is odd\n    if window_size % 2 == 0:\n        window_size += 1\n\n    # Apply noise and denoising\n    noisy_img = add_salt_and_pepper_noise(image_gray, noise_ratio)\n    denoised_img = median_filter(noisy_img, window_size)\n\n    # Create Plotly figures\n    fig_noisy = px.imshow(noisy_img, color_continuous_scale=\"gray\", title=\"Noisy Image\")\n    fig_denoised = px.imshow(\n        denoised_img, color_continuous_scale=\"gray\", title=\"Denoised Image\"\n    )\n\n    details = {\n        \"Noise Ratio\": noise_ratio,\n        \"Median Filter Window Size\": window_size,\n        \"Original Image Shape\": image_gray.shape,\n    }\n\n    return {\n        \"Noisy Image\": fig_noisy,\n        \"Denoised Image\": fig_denoised,\n        \"Details\": details,\n    }\n\n\nwith gr.Blocks(\n    title=\"Image Denoising via Median Filtering\",\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n) as demo:\n    image_input = gr.Image(label=\"Upload Image\", value=DEFAULT_IMAGE)\n    with gr.Row():\n        noise_ratio_slider = gr.Slider(\n            minimum=0.0, maximum=0.5, step=0.01, value=0.1, label=\"Noise Ratio\"\n        )\n        window_size_slider = gr.Slider(\n            minimum=3,\n            maximum=21,\n            step=2,\n            value=3,\n            label=\"Median Filter Window Size (odd)\",\n        )\n\n    process_button = gr.Button(\"Apply Denoising\")\n\n    with gr.Row():\n        output_noisy = gr.Plot(label=\"Noisy Image\")\n        output_denoised = gr.Plot(label=\"Denoised Image\")\n\n    output_details = gr.JSON(label=\"Processing Details\")\n\n    def process_wrapper(image, noise_ratio, window_size):\n        # If no image is uploaded, use a default image from skimage\n        if image is None:\n            from skimage import data\n\n            image = img_as_float(data.camera())\n        results = process_image(image, noise_ratio, int(window_size))\n        return results[\"Noisy Image\"], results[\"Denoised Image\"], results[\"Details\"]\n\n    process_button.click(\n        process_wrapper,\n        inputs=[image_input, noise_ratio_slider, window_size_slider],\n        outputs=[output_noisy, output_denoised, output_details],\n    )\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/03-image-processing---edge-detection.html",
    "href": "content/03-image-processing---edge-detection.html",
    "title": "Edge Detection",
    "section": "",
    "text": "Code\nimport numpy as np\nimport gradio as gr\nfrom skimage import color, img_as_float, data\nimport plotly.graph_objects as go"
  },
  {
    "objectID": "content/03-image-processing---edge-detection.html#introduction",
    "href": "content/03-image-processing---edge-detection.html#introduction",
    "title": "Edge Detection",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis notebook provides detailed implementations of key image processing techniques, focusing on enhancing and extracting image features through:\n\nEdge Detection:\n\nPrewitt and Sobel Methods: These operators are essential tools for edge detection through gradient approximation. Edge detection aims to identify boundaries within an image by applying filters in the horizontal and vertical directions.\n\n\nThe convolutional operation is utilized as follows:\n\\[ (f * h)(n) = \\sum_{k \\in \\mathbb{Z}^2} f(k)h(n-k), \\]\nwhere the gradient magnitude highlights edges:\n\\[ E = \\sqrt{(f_x)^2 + (f_y)^2}. \\]\n\nHistogram Equalization:\n\nEnhances image contrast by reallocating pixel intensity distributions. For a pixel intensity distribution with levels \\(r_k\\) and probabilities:\n\n\\[ p_k = \\frac{N_k}{N}, \\]\nthe new intensity mapping utilizes the cumulative distribution:\n\\[ F(i,j) = \\sum_{l=1}^{k} p_l \\quad \\text{for } (i,j)\\in \\Omega_k. \\]\n\nThese methods are foundational in computer vision for preparing and pre-processing images toward further analysis."
  },
  {
    "objectID": "content/03-image-processing---edge-detection.html#python-implementation",
    "href": "content/03-image-processing---edge-detection.html#python-implementation",
    "title": "Edge Detection",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\nTo understand the imageâ€™s features, the convolve2d function helps perform a 2D convolution, a crucial step for applying filtering operations to images:\n\n\nCode\ndef convolve2d(image: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n     Perform a 2D convolution between an image and a kernel.\n\n     The kernel is flipped to implement convolution as defined by\n     $$\n     (f * h)(n) = \\sum_{k \\in \\mathbb{Z}^2} f(k)h(n-k).\n    $$\n\n     Parameters:\n         image (np.ndarray): 2D input image.\n         kernel (np.ndarray): 2D filter kernel.\n\n     Returns:\n         np.ndarray: The convolved image with the same shape as input.\n    \"\"\"\n    h, w = image.shape\n    kh, kw = kernel.shape\n    pad_h, pad_w = kh // 2, kw // 2\n    padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode=\"edge\")\n    output = np.zeros_like(image, dtype=float)\n    # Flip the kernel for convolution\n    kernel_flip = np.flipud(np.fliplr(kernel))\n    for i in range(h):\n        for j in range(w):\n            region = padded[i : i + kh, j : j + kw]\n            output[i, j] = np.sum(region * kernel_flip)\n    return output\n\n\nThe Gaussian kernel and smoothing functions allow us to apply a Gaussian filter, smoothing the image to reduce noise before edge detection:\n\n\nCode\ndef gaussian_kernel(size: int, sigma: float) -&gt; np.ndarray:\n    \"\"\"\n    Generate a 2D Gaussian kernel.\n\n    Parameters:\n        size (int): The size of the kernel (should be odd).\n        sigma (float): Standard deviation of the Gaussian.\n\n    Returns:\n        np.ndarray: Normalized Gaussian kernel.\n    \"\"\"\n    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n    xx, yy = np.meshgrid(ax, ax)\n    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n    kernel = kernel / np.sum(kernel)\n    return kernel\n\n\ndef smooth_image(image: np.ndarray, kernel_size: int, sigma: float) -&gt; np.ndarray:\n    \"\"\"\n    Smooth an image using a Gaussian filter.\n\n    Parameters:\n        image (np.ndarray): 2D input image.\n        kernel_size (int): Size of the Gaussian kernel (odd integer).\n        sigma (float): Standard deviation for the Gaussian.\n\n    Returns:\n        np.ndarray: Smoothed image.\n    \"\"\"\n    kernel = gaussian_kernel(kernel_size, sigma)\n    return convolve2d(image, kernel)\n\n\nThresholding aids in converting edge detected images to binary, accentuating the detected edges:\n\n\nCode\ndef threshold_image(image: np.ndarray, threshold: float) -&gt; np.ndarray:\n    \"\"\"\n    Threshold an image to create a binary image.\n\n    Parameters:\n        image (np.ndarray): Input image.\n        threshold (float): Threshold value.\n\n    Returns:\n        np.ndarray: Binary image (values 0 or 1).\n    \"\"\"\n    return (image &gt;= threshold).astype(float)\n\n\nDefine Prewitt and Sobel edge detection filters, both acting on pixel intensity gradients:\n\n\nCode\nprewitt_x = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]], dtype=float)\nprewitt_y = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]], dtype=float)\n\nsobel_x = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=float)\nsobel_y = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=float)\n\n\nUtilize the filters for edge detection:\n\n\nCode\ndef prewitt_edge_detection(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Apply the Prewitt edge detection method.\n\n    Parameters:\n        image (np.ndarray): 2D grayscale image.\n\n    Returns:\n        np.ndarray: Edge magnitude image.\n    \"\"\"\n    gx = convolve2d(image, prewitt_x)\n    gy = convolve2d(image, prewitt_y)\n    return np.sqrt(gx**2 + gy**2)\n\n\ndef sobel_edge_detection(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Apply the Sobel edge detection method.\n\n    Parameters:\n        image (np.ndarray): 2D grayscale image.\n\n    Returns:\n        np.ndarray: Edge magnitude image.\n    \"\"\"\n    gx = convolve2d(image, sobel_x)\n    gy = convolve2d(image, sobel_y)\n    return np.sqrt(gx**2 + gy**2)\n\n\nImplement histogram equalization to enhance contrast:\n\n\nCode\ndef histogram_equalization(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Perform histogram equalization to enhance image contrast.\n\n    Parameters:\n        image (np.ndarray): 2D grayscale image with values in [0, 1].\n\n    Returns:\n        np.ndarray: Contrast-enhanced image.\n    \"\"\"\n    # Compute histogram and cumulative distribution function (CDF)\n    hist, bins = np.histogram(image.flatten(), bins=256, range=[0, 1])\n    cdf = hist.cumsum()\n    cdf_normalized = cdf / cdf[-1]\n    # Use linear interpolation of the CDF to map the original gray levels\n    equalized = np.interp(image.flatten(), bins[:-1], cdf_normalized).reshape(\n        image.shape\n    )\n    return equalized\n\n\nCreate interactive plotting for histograms, alongside edge detection and histogram equalization functionalities:\n\n\nCode\ndef compute_histogram(image: np.ndarray) -&gt; dict:\n    \"\"\"\n    Compute the histogram of an image and return as a JSON-serializable dictionary.\n\n    Parameters:\n        image (np.ndarray): 2D image with pixel values in [0, 1].\n\n    Returns:\n        dict: Dictionary containing histogram counts and bin edges.\n    \"\"\"\n    hist, bin_edges = np.histogram(image.flatten(), bins=256, range=(0, 1))\n    return {\"histogram_counts\": hist.tolist(), \"bin_edges\": bin_edges.tolist()}"
  },
  {
    "objectID": "content/03-image-processing---edge-detection.html#interactive-dashboard",
    "href": "content/03-image-processing---edge-detection.html#interactive-dashboard",
    "title": "Edge Detection",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\n\n\nCode\nDEFAULT_IMAGE = img_as_float(data.camera())\n\n\n\n\nCode\ndef compute_histogram(image: np.ndarray, bins: int = 256) -&gt; dict:\n    \"\"\"\n    Compute the histogram of an image.\n\n    Parameters:\n        image (np.ndarray): Input image\n        bins (int): Number of histogram bins\n\n    Returns:\n        dict: Dictionary containing histogram data\n    \"\"\"\n    hist, bin_edges = np.histogram(image, bins=bins, range=(0, 1))\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    return {\n        \"counts\": hist.tolist(),\n        \"bin_edges\": bin_edges.tolist(),\n        \"bin_centers\": bin_centers.tolist(),\n    }\n\n\ndef create_histogram_plot(hist_data: dict) -&gt; go.Figure:\n    \"\"\"\n    Create a Plotly histogram figure from the histogram data.\n\n    Parameters:\n        hist_data (dict): Dictionary containing histogram data\n\n    Returns:\n        go.Figure: Plotly figure object\n    \"\"\"\n    fig = go.Figure(\n        data=[\n            go.Bar(\n                x=hist_data[\"bin_centers\"],\n                y=hist_data[\"counts\"],\n                name=\"Pixel Intensity Distribution\",\n                marker_color=\"rgb(55, 83, 109)\",\n            )\n        ]\n    )\n\n    fig.update_layout(\n        title=\"Image Histogram\",\n        xaxis_title=\"Pixel Intensity\",\n        yaxis_title=\"Frequency\",\n        bargap=0.01,\n        template=\"plotly_white\",\n        showlegend=False,\n    )\n\n    return fig\n\n\ndef process_image(\n    image: np.ndarray,\n    method: str,\n    threshold: float = 0.1,\n    kernel_size: int = 3,\n    sigma: float = 1.0,\n) -&gt; tuple[np.ndarray, go.Figure]:\n    \"\"\"\n    Process the input image based on the selected method.\n\n    Parameters:\n        image (np.ndarray): Input image (RGB or grayscale). Expected values in [0,1].\n        method (str): Processing method.\n        threshold (float): Threshold value for binary edge detection.\n        kernel_size (int): Kernel size for Gaussian smoothing (odd integer).\n        sigma (float): Standard deviation for Gaussian smoothing.\n\n    Returns:\n        tuple[np.ndarray, go.Figure]: A tuple containing the processed image and the Plotly figure\n    \"\"\"\n    # Convert to float image in [0,1]\n    if image.ndim == 3:\n        image_gray = color.rgb2gray(image)\n    else:\n        image_gray = image.copy()\n\n    # Process according to the selected method\n    if method == \"Prewitt Edge Detection\":\n        proc_img = prewitt_edge_detection(image_gray)\n    elif method == \"Sobel Edge Detection\":\n        proc_img = sobel_edge_detection(image_gray)\n    elif method == \"Edge Thresholding (Sobel + Threshold)\":\n        edges = sobel_edge_detection(image_gray)\n        proc_img = threshold_image(edges, threshold)\n    elif method == \"Histogram Equalization\":\n        proc_img = histogram_equalization(image_gray)\n    elif method == \"Gaussian Smoothing\":\n        proc_img = smooth_image(image_gray, kernel_size, sigma)\n    else:\n        proc_img = image_gray  # Fallback: return original image\n\n    # Normalize the processed image to [0, 1] for display (if not already binary)\n    if proc_img.max() &gt; 1 or proc_img.min() &lt; 0:\n        proc_img = (proc_img - proc_img.min()) / (\n            proc_img.max() - proc_img.min() + 1e-8\n        )\n\n    hist_data = compute_histogram(proc_img)\n    plot_fig = create_histogram_plot(hist_data)\n    return proc_img, plot_fig\n\n\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"Edge Detection & Histogram Equalization\",\n) as demo:\n    with gr.Row():\n        image_input = gr.Image(label=\"Input Image\", type=\"numpy\", value=DEFAULT_IMAGE)\n        output_image = gr.Image(label=\"Processed Image\")\n\n    process_button = gr.Button(\"Process Image\")\n\n    method_choice = gr.Dropdown(\n        choices=[\n            \"Prewitt Edge Detection\",\n            \"Sobel Edge Detection\",\n            \"Edge Thresholding (Sobel + Threshold)\",\n            \"Histogram Equalization\",\n            \"Gaussian Smoothing\",\n        ],\n        label=\"Processing Method\",\n        value=\"Prewitt Edge Detection\",\n    )\n    threshold_slider = gr.Slider(\n        minimum=0.0,\n        maximum=1.0,\n        step=0.01,\n        label=\"Threshold (for Edge Thresholding)\",\n        value=0.1,\n    )\n    kernel_slider = gr.Slider(\n        minimum=3, maximum=15, step=2, label=\"Gaussian Kernel Size\", value=3\n    )\n    sigma_slider = gr.Slider(\n        minimum=0.1, maximum=5.0, step=0.1, label=\"Gaussian Sigma\", value=1.0\n    )\n    output_histogram = gr.Plot(label=\"Histogram\")\n\n    process_button.click(\n        fn=process_image,\n        inputs=[\n            image_input,\n            method_choice,\n            threshold_slider,\n            kernel_slider,\n            sigma_slider,\n        ],\n        outputs=[output_image, output_histogram],\n    )\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport numpy as np\nimport gradio as gr\nfrom skimage import color, img_as_float, data\nimport plotly.graph_objects as go\ndef convolve2d(image: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n     Perform a 2D convolution between an image and a kernel.\n\n     The kernel is flipped to implement convolution as defined by\n     $$\n     (f * h)(n) = \\sum_{k \\in \\mathbb{Z}^2} f(k)h(n-k).\n    $$\n\n     Parameters:\n         image (np.ndarray): 2D input image.\n         kernel (np.ndarray): 2D filter kernel.\n\n     Returns:\n         np.ndarray: The convolved image with the same shape as input.\n    \"\"\"\n    h, w = image.shape\n    kh, kw = kernel.shape\n    pad_h, pad_w = kh // 2, kw // 2\n    padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode=\"edge\")\n    output = np.zeros_like(image, dtype=float)\n    # Flip the kernel for convolution\n    kernel_flip = np.flipud(np.fliplr(kernel))\n    for i in range(h):\n        for j in range(w):\n            region = padded[i : i + kh, j : j + kw]\n            output[i, j] = np.sum(region * kernel_flip)\n    return output\ndef gaussian_kernel(size: int, sigma: float) -&gt; np.ndarray:\n    \"\"\"\n    Generate a 2D Gaussian kernel.\n\n    Parameters:\n        size (int): The size of the kernel (should be odd).\n        sigma (float): Standard deviation of the Gaussian.\n\n    Returns:\n        np.ndarray: Normalized Gaussian kernel.\n    \"\"\"\n    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n    xx, yy = np.meshgrid(ax, ax)\n    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n    kernel = kernel / np.sum(kernel)\n    return kernel\n\n\ndef smooth_image(image: np.ndarray, kernel_size: int, sigma: float) -&gt; np.ndarray:\n    \"\"\"\n    Smooth an image using a Gaussian filter.\n\n    Parameters:\n        image (np.ndarray): 2D input image.\n        kernel_size (int): Size of the Gaussian kernel (odd integer).\n        sigma (float): Standard deviation for the Gaussian.\n\n    Returns:\n        np.ndarray: Smoothed image.\n    \"\"\"\n    kernel = gaussian_kernel(kernel_size, sigma)\n    return convolve2d(image, kernel)\ndef threshold_image(image: np.ndarray, threshold: float) -&gt; np.ndarray:\n    \"\"\"\n    Threshold an image to create a binary image.\n\n    Parameters:\n        image (np.ndarray): Input image.\n        threshold (float): Threshold value.\n\n    Returns:\n        np.ndarray: Binary image (values 0 or 1).\n    \"\"\"\n    return (image &gt;= threshold).astype(float)\nprewitt_x = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]], dtype=float)\nprewitt_y = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]], dtype=float)\n\nsobel_x = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=float)\nsobel_y = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=float)\ndef prewitt_edge_detection(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Apply the Prewitt edge detection method.\n\n    Parameters:\n        image (np.ndarray): 2D grayscale image.\n\n    Returns:\n        np.ndarray: Edge magnitude image.\n    \"\"\"\n    gx = convolve2d(image, prewitt_x)\n    gy = convolve2d(image, prewitt_y)\n    return np.sqrt(gx**2 + gy**2)\n\n\ndef sobel_edge_detection(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Apply the Sobel edge detection method.\n\n    Parameters:\n        image (np.ndarray): 2D grayscale image.\n\n    Returns:\n        np.ndarray: Edge magnitude image.\n    \"\"\"\n    gx = convolve2d(image, sobel_x)\n    gy = convolve2d(image, sobel_y)\n    return np.sqrt(gx**2 + gy**2)\ndef histogram_equalization(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Perform histogram equalization to enhance image contrast.\n\n    Parameters:\n        image (np.ndarray): 2D grayscale image with values in [0, 1].\n\n    Returns:\n        np.ndarray: Contrast-enhanced image.\n    \"\"\"\n    # Compute histogram and cumulative distribution function (CDF)\n    hist, bins = np.histogram(image.flatten(), bins=256, range=[0, 1])\n    cdf = hist.cumsum()\n    cdf_normalized = cdf / cdf[-1]\n    # Use linear interpolation of the CDF to map the original gray levels\n    equalized = np.interp(image.flatten(), bins[:-1], cdf_normalized).reshape(\n        image.shape\n    )\n    return equalized\ndef compute_histogram(image: np.ndarray) -&gt; dict:\n    \"\"\"\n    Compute the histogram of an image and return as a JSON-serializable dictionary.\n\n    Parameters:\n        image (np.ndarray): 2D image with pixel values in [0, 1].\n\n    Returns:\n        dict: Dictionary containing histogram counts and bin edges.\n    \"\"\"\n    hist, bin_edges = np.histogram(image.flatten(), bins=256, range=(0, 1))\n    return {\"histogram_counts\": hist.tolist(), \"bin_edges\": bin_edges.tolist()}\nDEFAULT_IMAGE = img_as_float(data.camera())\ndef compute_histogram(image: np.ndarray, bins: int = 256) -&gt; dict:\n    \"\"\"\n    Compute the histogram of an image.\n\n    Parameters:\n        image (np.ndarray): Input image\n        bins (int): Number of histogram bins\n\n    Returns:\n        dict: Dictionary containing histogram data\n    \"\"\"\n    hist, bin_edges = np.histogram(image, bins=bins, range=(0, 1))\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    return {\n        \"counts\": hist.tolist(),\n        \"bin_edges\": bin_edges.tolist(),\n        \"bin_centers\": bin_centers.tolist(),\n    }\n\n\ndef create_histogram_plot(hist_data: dict) -&gt; go.Figure:\n    \"\"\"\n    Create a Plotly histogram figure from the histogram data.\n\n    Parameters:\n        hist_data (dict): Dictionary containing histogram data\n\n    Returns:\n        go.Figure: Plotly figure object\n    \"\"\"\n    fig = go.Figure(\n        data=[\n            go.Bar(\n                x=hist_data[\"bin_centers\"],\n                y=hist_data[\"counts\"],\n                name=\"Pixel Intensity Distribution\",\n                marker_color=\"rgb(55, 83, 109)\",\n            )\n        ]\n    )\n\n    fig.update_layout(\n        title=\"Image Histogram\",\n        xaxis_title=\"Pixel Intensity\",\n        yaxis_title=\"Frequency\",\n        bargap=0.01,\n        template=\"plotly_white\",\n        showlegend=False,\n    )\n\n    return fig\n\n\ndef process_image(\n    image: np.ndarray,\n    method: str,\n    threshold: float = 0.1,\n    kernel_size: int = 3,\n    sigma: float = 1.0,\n) -&gt; tuple[np.ndarray, go.Figure]:\n    \"\"\"\n    Process the input image based on the selected method.\n\n    Parameters:\n        image (np.ndarray): Input image (RGB or grayscale). Expected values in [0,1].\n        method (str): Processing method.\n        threshold (float): Threshold value for binary edge detection.\n        kernel_size (int): Kernel size for Gaussian smoothing (odd integer).\n        sigma (float): Standard deviation for Gaussian smoothing.\n\n    Returns:\n        tuple[np.ndarray, go.Figure]: A tuple containing the processed image and the Plotly figure\n    \"\"\"\n    # Convert to float image in [0,1]\n    if image.ndim == 3:\n        image_gray = color.rgb2gray(image)\n    else:\n        image_gray = image.copy()\n\n    # Process according to the selected method\n    if method == \"Prewitt Edge Detection\":\n        proc_img = prewitt_edge_detection(image_gray)\n    elif method == \"Sobel Edge Detection\":\n        proc_img = sobel_edge_detection(image_gray)\n    elif method == \"Edge Thresholding (Sobel + Threshold)\":\n        edges = sobel_edge_detection(image_gray)\n        proc_img = threshold_image(edges, threshold)\n    elif method == \"Histogram Equalization\":\n        proc_img = histogram_equalization(image_gray)\n    elif method == \"Gaussian Smoothing\":\n        proc_img = smooth_image(image_gray, kernel_size, sigma)\n    else:\n        proc_img = image_gray  # Fallback: return original image\n\n    # Normalize the processed image to [0, 1] for display (if not already binary)\n    if proc_img.max() &gt; 1 or proc_img.min() &lt; 0:\n        proc_img = (proc_img - proc_img.min()) / (\n            proc_img.max() - proc_img.min() + 1e-8\n        )\n\n    hist_data = compute_histogram(proc_img)\n    plot_fig = create_histogram_plot(hist_data)\n    return proc_img, plot_fig\n\n\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"Edge Detection & Histogram Equalization\",\n) as demo:\n    with gr.Row():\n        image_input = gr.Image(label=\"Input Image\", type=\"numpy\", value=DEFAULT_IMAGE)\n        output_image = gr.Image(label=\"Processed Image\")\n\n    process_button = gr.Button(\"Process Image\")\n\n    method_choice = gr.Dropdown(\n        choices=[\n            \"Prewitt Edge Detection\",\n            \"Sobel Edge Detection\",\n            \"Edge Thresholding (Sobel + Threshold)\",\n            \"Histogram Equalization\",\n            \"Gaussian Smoothing\",\n        ],\n        label=\"Processing Method\",\n        value=\"Prewitt Edge Detection\",\n    )\n    threshold_slider = gr.Slider(\n        minimum=0.0,\n        maximum=1.0,\n        step=0.01,\n        label=\"Threshold (for Edge Thresholding)\",\n        value=0.1,\n    )\n    kernel_slider = gr.Slider(\n        minimum=3, maximum=15, step=2, label=\"Gaussian Kernel Size\", value=3\n    )\n    sigma_slider = gr.Slider(\n        minimum=0.1, maximum=5.0, step=0.1, label=\"Gaussian Sigma\", value=1.0\n    )\n    output_histogram = gr.Plot(label=\"Histogram\")\n\n    process_button.click(\n        fn=process_image,\n        inputs=[\n            image_input,\n            method_choice,\n            threshold_slider,\n            kernel_slider,\n            sigma_slider,\n        ],\n        outputs=[output_image, output_histogram],\n    )\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/04-high-dimensional-data.html",
    "href": "content/04-high-dimensional-data.html",
    "title": "High-dimensional Data Analysis",
    "section": "",
    "text": "Code\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport gradio as gr"
  },
  {
    "objectID": "content/04-high-dimensional-data.html#introduction",
    "href": "content/04-high-dimensional-data.html#introduction",
    "title": "High-dimensional Data Analysis",
    "section": "1 Introduction",
    "text": "1 Introduction\nHigh-dimensional data analysis is essential in modern statistics and machine learning. It involves understanding data where the number of features (dimensions) greatly exceeds the number of samples. This notebook explores several fundamental concepts and theoretical tools to navigate and analyze high-dimensional datasets effectively:\n\nChebyshevâ€™s Inequality: A statistical bound that estimates the probability deviations of a random variable from its mean, applicable to any distribution with a finite variance. Formally:\n\\[\nP(|X - EX| \\geq t) \\leq \\frac{\\text{Var}(X)}{t^2}\n\\]\nWe demonstrate this using a uniformly distributed random variable to show how bounds hold against actual probability calculations.\nWeak Law of Large Numbers (WLLN):\n\nThis principle asserts that, with an increasing number of independent samples, the sample mean approaches the expected value with high probability:\n\\[\n\\lim_{n \\to \\infty} P(|S_n - \\mu| \\geq \\epsilon) = 0, \\quad \\forall \\epsilon &gt; 0\n\\]\nWe illustrate this with Bernoulli trials to observe convergence behavior.\n\nHigh-dimensional Geometry:\n\nAs dimensionality increases, random vectors tend toward orthogonality (they become nearly perpendicular). For independent vectors drawn from a normal distribution:\n\\[\nP\\left(\\frac{| \\langle X, Y \\rangle |}{||X|| \\cdot ||Y||} \\geq t \\right) \\leq \\frac{1}{dt^2}\n\\]\nAs dimensions grow, vector angles tend to small values infrequently, highlighting dimensionalityâ€™s role.\n\nJohnson-Lindenstrauss Lemma:\n\nAn important dimensionality reduction result, ensuring that high-dimensional data can be projected into lower-dimensional spaces with minimal distortion of pairwise distances."
  },
  {
    "objectID": "content/04-high-dimensional-data.html#python-implementation",
    "href": "content/04-high-dimensional-data.html#python-implementation",
    "title": "High-dimensional Data Analysis",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\nImplementing Chebyshevâ€™s inequality for a uniformly distributed random variable, we compare actual and theoretical probabilities:\n\n\nCode\ndef chebyshev_uniform_demo(t: float) -&gt; tuple[float, float, dict]:\n    \"\"\"\n    Demonstrates Chebyshev inequality for X ~ Uniform[0,1]\n    Returns (actual_prob, bound_prob, stats)\n    \"\"\"\n    actual_prob = 1 - 2 * t if 0 &lt; t &lt; 0.5 else 0.0\n    var = 1 / 12\n    bound_prob = min(var / t**2, 1.0) if t &gt; 0 else 1.0\n    stats = {\n        \"mean\": 0.5,\n        \"variance\": var,\n        \"threshold\": t,\n        \"actual_probability\": actual_prob,\n        \"chebyshev_bound\": bound_prob,\n    }\n    return actual_prob, bound_prob, stats\n\n\nSimulate the Weak Law of Large Numbers (WLLN) using Bernoulli trials and observe how sample means approach the expected value:\n\n\nCode\ndef wlln_simulation(n: int, num_samples: int = 1000) -&gt; dict:\n    \"\"\"Simulates Weak Law of Large Numbers for Bernoulli trials\"\"\"\n    samples = np.random.binomial(1, 0.5, (num_samples, n))\n    sample_means = samples.mean(axis=1)\n    stats = {\n        \"expected_mean\": 0.5,\n        \"sample_means_mean\": sample_means.mean(),\n        \"sample_means_var\": sample_means.var(),\n        \"chebyshev_bound\": 1 / (4 * n * 0.05**2),  # For Îµ=0.05\n    }\n    return stats\n\n\nUnderstand the geometry of high dimensions by checking the orthogonality between random vectors:\n\n\nCode\ndef high_dim_orthogonality(d: int, num_pairs: int = 1000) -&gt; dict:\n    \"\"\"Calculates inner product statistics in high dimensions\"\"\"\n    X = np.random.normal(0, 1, (num_pairs, d))\n    Y = np.random.normal(0, 1, (num_pairs, d))\n    norms_X = np.linalg.norm(X, axis=1)\n    norms_Y = np.linalg.norm(Y, axis=1)\n    cos_theta = np.sum(X * Y, axis=1) / (norms_X * norms_Y)\n    stats = {\n        \"mean_angle\": np.mean(np.arccos(cos_theta)),\n        \"prob_above_0.1\": np.mean(np.abs(cos_theta) &gt; 0.1),\n        \"chebyshev_bound\": 1 / (d * 0.1**2),\n    }\n    return stats\n\n\nPerform random projection as per the Johnson-Lindenstrauss Lemma to dimensionality reduction while preserving distances:\n\n\nCode\ndef johnson_lindenstrauss_project(X: np.ndarray, k: int) -&gt; np.ndarray:\n    \"\"\"Random projection matrix for JL Lemma\"\"\"\n    d = X.shape[1]\n    Q = np.random.normal(0, 1 / np.sqrt(k), (d, k))\n    return X @ Q"
  },
  {
    "objectID": "content/04-high-dimensional-data.html#interactive-dashboard",
    "href": "content/04-high-dimensional-data.html#interactive-dashboard",
    "title": "High-dimensional Data Analysis",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\nUsing Gradio, this interactive interface allows exploring theoretical concepts with adjustable parameters and visualization:\n\n\nCode\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"High-Dimensional Data Behavior\",\n) as demo:\n    with gr.Tab(\"Chebyshev Inequality\"):\n        t_input = gr.Slider(0.01, 0.49, value=0.2, label=\"Threshold t\")\n        cheb_plot = gr.Plot()\n        cheb_json = gr.JSON()\n\n        def update_cheb(t):\n            actual, bound, stats = chebyshev_uniform_demo(t)\n            fig = go.Figure()\n            fig.add_trace(\n                go.Scatter(x=np.linspace(0, 1, 100), y=[0.5] * 100, name=\"Mean\")\n            )\n            fig.add_vrect(\n                x0=0.5 - t,\n                x1=0.5 + t,\n                fillcolor=\"green\",\n                opacity=0.2,\n                name=\"Acceptance\",\n            )\n            fig.update_layout(title=\"Probability Concentration: Actual vs Bound\")\n            cheb_json = stats\n            return fig, cheb_json\n\n        t_input.change(update_cheb, t_input, [cheb_plot, cheb_json])\n\n    with gr.Tab(\"Weak Law of Large Numbers\"):\n        n_input = gr.Slider(10, 1000, value=100, step=10, label=\"Sample size n\")\n        wlln_plot = gr.Plot()\n\n        def update_wlln(n):\n            means = [wlln_simulation(int(n))[\"sample_means_mean\"] for _ in range(100)]\n            fig = px.line(\n                x=range(100),\n                y=means,\n                labels={\"x\": \"Trial\", \"y\": \"Sample Mean\"},\n                title=\"Convergence of Sample Means\",\n            )\n            fig.add_hline(y=0.5, line_dash=\"dash\")\n            return fig\n\n        n_input.change(update_wlln, n_input, wlln_plot)\n\n    with gr.Tab(\"High-D Orthogonality\"):\n        dim_input = gr.Slider(2, 1000, value=100, label=\"Dimension d\")\n        angle_plot = gr.Plot()\n        angle_stats = gr.JSON()\n\n        def update_angles(d):\n            stats = high_dim_orthogonality(int(d))\n            angles = np.random.normal(0, 1 / np.sqrt(d), 1000)\n            fig = px.histogram(\n                angles, nbins=50, title=\"Distribution of cosÎ¸ in High Dimensions\"\n            )\n            angle_stats = stats\n            return fig, angle_stats\n\n        dim_input.change(update_angles, dim_input, [angle_plot, angle_stats])\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport gradio as gr\ndef chebyshev_uniform_demo(t: float) -&gt; tuple[float, float, dict]:\n    \"\"\"\n    Demonstrates Chebyshev inequality for X ~ Uniform[0,1]\n    Returns (actual_prob, bound_prob, stats)\n    \"\"\"\n    actual_prob = 1 - 2 * t if 0 &lt; t &lt; 0.5 else 0.0\n    var = 1 / 12\n    bound_prob = min(var / t**2, 1.0) if t &gt; 0 else 1.0\n    stats = {\n        \"mean\": 0.5,\n        \"variance\": var,\n        \"threshold\": t,\n        \"actual_probability\": actual_prob,\n        \"chebyshev_bound\": bound_prob,\n    }\n    return actual_prob, bound_prob, stats\ndef wlln_simulation(n: int, num_samples: int = 1000) -&gt; dict:\n    \"\"\"Simulates Weak Law of Large Numbers for Bernoulli trials\"\"\"\n    samples = np.random.binomial(1, 0.5, (num_samples, n))\n    sample_means = samples.mean(axis=1)\n    stats = {\n        \"expected_mean\": 0.5,\n        \"sample_means_mean\": sample_means.mean(),\n        \"sample_means_var\": sample_means.var(),\n        \"chebyshev_bound\": 1 / (4 * n * 0.05**2),  # For Îµ=0.05\n    }\n    return stats\ndef high_dim_orthogonality(d: int, num_pairs: int = 1000) -&gt; dict:\n    \"\"\"Calculates inner product statistics in high dimensions\"\"\"\n    X = np.random.normal(0, 1, (num_pairs, d))\n    Y = np.random.normal(0, 1, (num_pairs, d))\n    norms_X = np.linalg.norm(X, axis=1)\n    norms_Y = np.linalg.norm(Y, axis=1)\n    cos_theta = np.sum(X * Y, axis=1) / (norms_X * norms_Y)\n    stats = {\n        \"mean_angle\": np.mean(np.arccos(cos_theta)),\n        \"prob_above_0.1\": np.mean(np.abs(cos_theta) &gt; 0.1),\n        \"chebyshev_bound\": 1 / (d * 0.1**2),\n    }\n    return stats\ndef johnson_lindenstrauss_project(X: np.ndarray, k: int) -&gt; np.ndarray:\n    \"\"\"Random projection matrix for JL Lemma\"\"\"\n    d = X.shape[1]\n    Q = np.random.normal(0, 1 / np.sqrt(k), (d, k))\n    return X @ Q\nwith gr.Blocks(\n    css=\"\"\"gradio-app {background: #222222 !important}\"\"\",\n    title=\"High-Dimensional Data Behavior\",\n) as demo:\n    with gr.Tab(\"Chebyshev Inequality\"):\n        t_input = gr.Slider(0.01, 0.49, value=0.2, label=\"Threshold t\")\n        cheb_plot = gr.Plot()\n        cheb_json = gr.JSON()\n\n        def update_cheb(t):\n            actual, bound, stats = chebyshev_uniform_demo(t)\n            fig = go.Figure()\n            fig.add_trace(\n                go.Scatter(x=np.linspace(0, 1, 100), y=[0.5] * 100, name=\"Mean\")\n            )\n            fig.add_vrect(\n                x0=0.5 - t,\n                x1=0.5 + t,\n                fillcolor=\"green\",\n                opacity=0.2,\n                name=\"Acceptance\",\n            )\n            fig.update_layout(title=\"Probability Concentration: Actual vs Bound\")\n            cheb_json = stats\n            return fig, cheb_json\n\n        t_input.change(update_cheb, t_input, [cheb_plot, cheb_json])\n\n    with gr.Tab(\"Weak Law of Large Numbers\"):\n        n_input = gr.Slider(10, 1000, value=100, step=10, label=\"Sample size n\")\n        wlln_plot = gr.Plot()\n\n        def update_wlln(n):\n            means = [wlln_simulation(int(n))[\"sample_means_mean\"] for _ in range(100)]\n            fig = px.line(\n                x=range(100),\n                y=means,\n                labels={\"x\": \"Trial\", \"y\": \"Sample Mean\"},\n                title=\"Convergence of Sample Means\",\n            )\n            fig.add_hline(y=0.5, line_dash=\"dash\")\n            return fig\n\n        n_input.change(update_wlln, n_input, wlln_plot)\n\n    with gr.Tab(\"High-D Orthogonality\"):\n        dim_input = gr.Slider(2, 1000, value=100, label=\"Dimension d\")\n        angle_plot = gr.Plot()\n        angle_stats = gr.JSON()\n\n        def update_angles(d):\n            stats = high_dim_orthogonality(int(d))\n            angles = np.random.normal(0, 1 / np.sqrt(d), 1000)\n            fig = px.histogram(\n                angles, nbins=50, title=\"Distribution of cosÎ¸ in High Dimensions\"\n            )\n            angle_stats = stats\n            return fig, angle_stats\n\n        dim_input.change(update_angles, dim_input, [angle_plot, angle_stats])\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  },
  {
    "objectID": "content/02-data-compression---huffman.html",
    "href": "content/02-data-compression---huffman.html",
    "title": "Huffman Encoding",
    "section": "",
    "text": "Code\nfrom dataclasses import dataclass\nimport gradio as gr"
  },
  {
    "objectID": "content/02-data-compression---huffman.html#introduction",
    "href": "content/02-data-compression---huffman.html#introduction",
    "title": "Huffman Encoding",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis notebook provides a comprehensive implementation of Huffman Encoding in Python, a classical algorithm for lossless data compression. Huffman encoding assigns variable-length codes to input characters, ensuring that more frequent symbols have shorter codes, and less frequent symbols have longer ones. The efficiency of encoding is achieved by minimizing the total number of bits needed for any given set of symbols based on their frequency:\n\\[\n\\text{Total Bits} = \\sum_{i} f_i \\cdot l_i,\n\\]\nwhere \\(f_i\\) is the frequency of symbol \\(i\\), and \\(l_i\\) is the length of its code. The core of Huffman encoding is building a Huffman tree, a binary tree with unique prefixes, optimizing the data compression process.\nThrough this notebook, you will explore:\n\nFrequency Table Construction: Analyze character occurrence frequencies in the input string.\nHuffman Tree Building: Develop a tree structure that guides efficient encoding.\nBinary Encoding Generation: Assign optimal binary codes based on tree traversal.\nTree Visualization: Use Mermaid diagrams for intuitive understanding.\nInteractive Gradio Interface: Engage with visualizations and encoding results interactively.\n\nEnter a text string in the interactive interface below to observe how Huffman encoding dynamically compresses data."
  },
  {
    "objectID": "content/02-data-compression---huffman.html#python-implementation",
    "href": "content/02-data-compression---huffman.html#python-implementation",
    "title": "Huffman Encoding",
    "section": "2 Python Implementation",
    "text": "2 Python Implementation\nThe implementation begins with defining data structures for the components of a Huffman Tree. Weâ€™ll utilize Pythonâ€™s dataclass to create nodes and leaves efficiently, necessary for constructing the Huffman tree:\n\n\nCode\n@dataclass\nclass HuffmanLeaf:\n    char: str\n    freq: int\n\n\n@dataclass\nclass HuffmanNode:\n    left: \"HuffmanTree\"\n    right: \"HuffmanTree\"\n    freq: int\n\n\nHuffmanTree = HuffmanLeaf | HuffmanNode\n\n\nThe following functions enable us to calculate frequency distribution, build the Huffman tree, and generate optimal binary codes for each symbol:\n\n\nCode\ndef make_freq_dict(s: str) -&gt; dict[str, int]:\n    \"\"\"\n    Generate a frequency dictionary for the characters in the input string.\n\n    Args:\n        s (str): The input string.\n\n    Returns:\n        dict[str, int]: A dictionary mapping each character to its frequency.\n    \"\"\"\n    freq = {}\n    for ch in s:\n        freq[ch] = freq.get(ch, 0) + 1\n    return freq\n\n\ndef build_huffman_tree(freq_dict: dict[str, int]) -&gt; HuffmanTree:\n    \"\"\"\n    Construct the Huffman tree from the frequency dictionary.\n\n    The algorithm repeatedly takes the two trees with the smallest frequencies,\n    merges them into a new node, and continues until one tree remains.\n\n    Args:\n        freq_dict (dict[str, int]): A dictionary mapping characters to frequencies.\n\n    Returns:\n        HuffmanTree: The root of the Huffman tree.\n    \"\"\"\n    trees: list[HuffmanTree] = [HuffmanLeaf(ch, fq) for ch, fq in freq_dict.items()]\n    while len(trees) &gt; 1:\n        trees.sort(key=lambda x: x.freq, reverse=True)  # smallest frequency at the end\n        left = trees.pop()  # smallest\n        right = trees.pop()  # second smallest\n        trees.append(HuffmanNode(left, right, left.freq + right.freq))\n    return trees[0]\n\n\ndef generate_encoding(tree: HuffmanTree, prefix: str = \"\") -&gt; dict[str, str]:\n    \"\"\"\n    Recursively traverse the Huffman tree to generate the binary encoding for each character.\n\n    By convention, the left branch appends '0' and the right branch appends '1'.\n\n    Args:\n        tree (HuffmanTree): The Huffman tree.\n        prefix (str, optional): The current prefix (binary code) accumulated during recursion.\n\n    Returns:\n        dict[str, str]: A dictionary mapping each character to its binary code.\n    \"\"\"\n    if isinstance(tree, HuffmanLeaf):\n        # Handle the edge case where the tree consists of a single node.\n        return {tree.char: prefix or \"0\"}\n    else:\n        encoding: dict[str, str] = {}\n        encoding.update(generate_encoding(tree.left, prefix + \"0\"))\n        encoding.update(generate_encoding(tree.right, prefix + \"1\"))\n        return encoding\n\n\nTo better visualize the Huffman Tree, we use Mermaid diagrams, which provide a straightforward representation of the branching structure:\n\n\nCode\ndef tree_to_mermaid(tree: HuffmanTree) -&gt; str:\n    \"\"\"\n    Generate a Mermaid diagram (in Markdown code block format) representing the Huffman tree.\n\n    The diagram uses a top-down (TD) flowchart where each internal node shows the combined frequency,\n    and each leaf node shows the character and its frequency.\n\n    Args:\n        tree (HuffmanTree): The Huffman tree.\n\n    Returns:\n        str: A string containing the Mermaid diagram.\n    \"\"\"\n    nodes = []\n    edges = []\n    counter = 0\n\n    def dfs(t: HuffmanTree, node_id: int):\n        nonlocal counter\n        if isinstance(t, HuffmanLeaf):\n            nodes.append(f'node{node_id}[\"{t.char} | {t.freq}\"]')\n        else:\n            nodes.append(f'node{node_id}[\"{t.freq}\"]')\n            counter += 1\n            left_id = counter\n            dfs(t.left, left_id)\n            edges.append(f'node{node_id} -- \"0\" --&gt; node{left_id}')\n            counter += 1\n            right_id = counter\n            dfs(t.right, right_id)\n            edges.append(f'node{node_id} -- \"1\" --&gt; node{right_id}')\n\n    dfs(tree, 0)\n    mermaid_str = \"flowchart TD\\n\" + \"\\n\".join(nodes + edges)\n    return mermaid_str"
  },
  {
    "objectID": "content/02-data-compression---huffman.html#interactive-dashboard",
    "href": "content/02-data-compression---huffman.html#interactive-dashboard",
    "title": "Huffman Encoding",
    "section": "3 Interactive Dashboard",
    "text": "3 Interactive Dashboard\nWe will now create an interactive Gradio interface, allowing you to input text strings to see their Huffman encoding. The dashboard provides:\n\nA breakdown of character frequencies and their encodings.\nVisualization of the Huffman Tree.\nComparison of encoded message length versus the original.\n\n\n\nCode\ndef create_mermaid_flowchart(mermaid_spec: str, height=\"400px\") -&gt; str:\n    mermaid_iframe = f\"\"\"\n    &lt;iframe srcdoc='\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js\"&gt;&lt;/script&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"mermaid\"&gt;\n            {mermaid_spec}\n        &lt;/div&gt;\n        &lt;script&gt;mermaid.initialize({{startOnLoad:true}});&lt;/script&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    ' style=\"width:100%; height:{height}; border:none\"&gt;\n    &lt;/iframe&gt;\n    \"\"\"\n    return mermaid_iframe\n\n\ndef process_input(input_str: str) -&gt; tuple[dict, str]:\n    \"\"\"\n    Process the input string to compute Huffman encoding details.\n\n    The function performs the following steps:\n      1. Computes the frequency dictionary of the input characters.\n      2. Builds the Huffman tree from these frequencies.\n      3. Generates the binary encoding for each character.\n      4. Calculates the total number of bits needed to encode the input.\n      5. Produces a Mermaid diagram of the Huffman tree.\n\n    Returns:\n      - A JSON-like dictionary containing the frequency table, encoding, and total bits.\n      - A Markdown string with the Mermaid diagram for the Huffman tree.\n\n    Args:\n        input_str (str): The input text string.\n\n    Returns:\n        tuple[dict, str, Image.Image]: The computed results.\n    \"\"\"\n    # 1. Frequency table\n    freq_dict = make_freq_dict(input_str)\n\n    # 2. Build Huffman tree\n    tree = build_huffman_tree(freq_dict)\n\n    # 3. Generate binary encoding for each character\n    encoding = generate_encoding(tree)\n\n    # 4. Calculate total bits required\n    total_bits = sum(freq_dict[ch] * len(encoding[ch]) for ch in freq_dict)\n    result_json = {\n        \"frequency_table\": freq_dict,\n        \"encoding\": encoding,\n        \"total_bits\": total_bits,\n    }\n\n    # 5. Generate Mermaid diagram for the Huffman tree\n    mermaid_diagram = tree_to_mermaid(tree)\n    mermaid_html = create_mermaid_flowchart(mermaid_diagram)\n\n    return result_json, mermaid_html\n\n\n\n\nCode\nwith gr.Blocks(css=\"\"\"gradio-app {background: #222222 !important}\"\"\") as demo:\n    gr.Markdown(\"# Huffman Encoding\")\n\n    # Input area (stacks vertically on mobile)\n    input_text = gr.Textbox(\n        lines=4, placeholder=\"Enter text here...\", label=\"Input Text\"\n    )\n\n    # Examples for quick testing\n    gr.Examples(\n        examples=[\n            [\"hello world\"],\n            [\"the quick brown fox jumps over the lazy dog\"],\n            [\n                \"ffcebcffcafffaedbfebffdedfdecbfffcfeeecfdfcffcbfcffeadfffedddffddbcfbcfefffbdfefbeefffffcffffefdefaa\"\n            ],\n            [\"ddcdeddabedebcdced\"],\n            [\"sdasaakaakkka\"],\n        ],\n        inputs=input_text,\n        label=\"Try an example\",\n    )\n\n    # A button to trigger processing\n    process_button = gr.Button(\"Encode\")\n\n    # Outputs: JSON details and Mermaid diagram\n    output_html = gr.HTML(\n        label=\"Huffman Tree (Mermaid Diagram)\", container=True, show_label=True\n    )\n    output_json = gr.JSON(label=\"Huffman Encoding Details\")\n\n    # Link the button click to the processing function\n    process_button.click(\n        fn=process_input, inputs=input_text, outputs=[output_json, output_html]\n    )\n\n\n\n\nCode\ndemo.launch(pwa=True, show_api=False, show_error=True)\n\n\n\n\nCode\n# Output of this cell set dynamically in Quarto filter step\n\n\n\n    \n        \n        \n        \n        \nimport micropip\nawait micropip.install('plotly==5.24.1');\n\n\nfrom dataclasses import dataclass\nimport gradio as gr\n@dataclass\nclass HuffmanLeaf:\n    char: str\n    freq: int\n\n\n@dataclass\nclass HuffmanNode:\n    left: \"HuffmanTree\"\n    right: \"HuffmanTree\"\n    freq: int\n\n\nHuffmanTree = HuffmanLeaf | HuffmanNode\ndef make_freq_dict(s: str) -&gt; dict[str, int]:\n    \"\"\"\n    Generate a frequency dictionary for the characters in the input string.\n\n    Args:\n        s (str): The input string.\n\n    Returns:\n        dict[str, int]: A dictionary mapping each character to its frequency.\n    \"\"\"\n    freq = {}\n    for ch in s:\n        freq[ch] = freq.get(ch, 0) + 1\n    return freq\n\n\ndef build_huffman_tree(freq_dict: dict[str, int]) -&gt; HuffmanTree:\n    \"\"\"\n    Construct the Huffman tree from the frequency dictionary.\n\n    The algorithm repeatedly takes the two trees with the smallest frequencies,\n    merges them into a new node, and continues until one tree remains.\n\n    Args:\n        freq_dict (dict[str, int]): A dictionary mapping characters to frequencies.\n\n    Returns:\n        HuffmanTree: The root of the Huffman tree.\n    \"\"\"\n    trees: list[HuffmanTree] = [HuffmanLeaf(ch, fq) for ch, fq in freq_dict.items()]\n    while len(trees) &gt; 1:\n        trees.sort(key=lambda x: x.freq, reverse=True)  # smallest frequency at the end\n        left = trees.pop()  # smallest\n        right = trees.pop()  # second smallest\n        trees.append(HuffmanNode(left, right, left.freq + right.freq))\n    return trees[0]\n\n\ndef generate_encoding(tree: HuffmanTree, prefix: str = \"\") -&gt; dict[str, str]:\n    \"\"\"\n    Recursively traverse the Huffman tree to generate the binary encoding for each character.\n\n    By convention, the left branch appends '0' and the right branch appends '1'.\n\n    Args:\n        tree (HuffmanTree): The Huffman tree.\n        prefix (str, optional): The current prefix (binary code) accumulated during recursion.\n\n    Returns:\n        dict[str, str]: A dictionary mapping each character to its binary code.\n    \"\"\"\n    if isinstance(tree, HuffmanLeaf):\n        # Handle the edge case where the tree consists of a single node.\n        return {tree.char: prefix or \"0\"}\n    else:\n        encoding: dict[str, str] = {}\n        encoding.update(generate_encoding(tree.left, prefix + \"0\"))\n        encoding.update(generate_encoding(tree.right, prefix + \"1\"))\n        return encoding\ndef tree_to_mermaid(tree: HuffmanTree) -&gt; str:\n    \"\"\"\n    Generate a Mermaid diagram (in Markdown code block format) representing the Huffman tree.\n\n    The diagram uses a top-down (TD) flowchart where each internal node shows the combined frequency,\n    and each leaf node shows the character and its frequency.\n\n    Args:\n        tree (HuffmanTree): The Huffman tree.\n\n    Returns:\n        str: A string containing the Mermaid diagram.\n    \"\"\"\n    nodes = []\n    edges = []\n    counter = 0\n\n    def dfs(t: HuffmanTree, node_id: int):\n        nonlocal counter\n        if isinstance(t, HuffmanLeaf):\n            nodes.append(f'node{node_id}[\"{t.char} | {t.freq}\"]')\n        else:\n            nodes.append(f'node{node_id}[\"{t.freq}\"]')\n            counter += 1\n            left_id = counter\n            dfs(t.left, left_id)\n            edges.append(f'node{node_id} -- \"0\" --&gt; node{left_id}')\n            counter += 1\n            right_id = counter\n            dfs(t.right, right_id)\n            edges.append(f'node{node_id} -- \"1\" --&gt; node{right_id}')\n\n    dfs(tree, 0)\n    mermaid_str = \"flowchart TD\\n\" + \"\\n\".join(nodes + edges)\n    return mermaid_str\ndef create_mermaid_flowchart(mermaid_spec: str, height=\"400px\") -&gt; str:\n    mermaid_iframe = f\"\"\"\n    &lt;iframe srcdoc='\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js\"&gt;&lt;/script&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"mermaid\"&gt;\n            {mermaid_spec}\n        &lt;/div&gt;\n        &lt;script&gt;mermaid.initialize({{startOnLoad:true}});&lt;/script&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    ' style=\"width:100%; height:{height}; border:none\"&gt;\n    &lt;/iframe&gt;\n    \"\"\"\n    return mermaid_iframe\n\n\ndef process_input(input_str: str) -&gt; tuple[dict, str]:\n    \"\"\"\n    Process the input string to compute Huffman encoding details.\n\n    The function performs the following steps:\n      1. Computes the frequency dictionary of the input characters.\n      2. Builds the Huffman tree from these frequencies.\n      3. Generates the binary encoding for each character.\n      4. Calculates the total number of bits needed to encode the input.\n      5. Produces a Mermaid diagram of the Huffman tree.\n\n    Returns:\n      - A JSON-like dictionary containing the frequency table, encoding, and total bits.\n      - A Markdown string with the Mermaid diagram for the Huffman tree.\n\n    Args:\n        input_str (str): The input text string.\n\n    Returns:\n        tuple[dict, str, Image.Image]: The computed results.\n    \"\"\"\n    # 1. Frequency table\n    freq_dict = make_freq_dict(input_str)\n\n    # 2. Build Huffman tree\n    tree = build_huffman_tree(freq_dict)\n\n    # 3. Generate binary encoding for each character\n    encoding = generate_encoding(tree)\n\n    # 4. Calculate total bits required\n    total_bits = sum(freq_dict[ch] * len(encoding[ch]) for ch in freq_dict)\n    result_json = {\n        \"frequency_table\": freq_dict,\n        \"encoding\": encoding,\n        \"total_bits\": total_bits,\n    }\n\n    # 5. Generate Mermaid diagram for the Huffman tree\n    mermaid_diagram = tree_to_mermaid(tree)\n    mermaid_html = create_mermaid_flowchart(mermaid_diagram)\n\n    return result_json, mermaid_html\nwith gr.Blocks(css=\"\"\"gradio-app {background: #222222 !important}\"\"\") as demo:\n    gr.Markdown(\"# Huffman Encoding\")\n\n    # Input area (stacks vertically on mobile)\n    input_text = gr.Textbox(\n        lines=4, placeholder=\"Enter text here...\", label=\"Input Text\"\n    )\n\n    # Examples for quick testing\n    gr.Examples(\n        examples=[\n            [\"hello world\"],\n            [\"the quick brown fox jumps over the lazy dog\"],\n            [\n                \"ffcebcffcafffaedbfebffdedfdecbfffcfeeecfdfcffcbfcffeadfffedddffddbcfbcfefffbdfefbeefffffcffffefdefaa\"\n            ],\n            [\"ddcdeddabedebcdced\"],\n            [\"sdasaakaakkka\"],\n        ],\n        inputs=input_text,\n        label=\"Try an example\",\n    )\n\n    # A button to trigger processing\n    process_button = gr.Button(\"Encode\")\n\n    # Outputs: JSON details and Mermaid diagram\n    output_html = gr.HTML(\n        label=\"Huffman Tree (Mermaid Diagram)\", container=True, show_label=True\n    )\n    output_json = gr.JSON(label=\"Huffman Encoding Details\")\n\n    # Link the button click to the processing function\n    process_button.click(\n        fn=process_input, inputs=input_text, outputs=[output_json, output_html]\n    )\ndemo.launch(pwa=True, show_api=False, show_error=True)"
  }
]